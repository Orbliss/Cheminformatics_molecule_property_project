{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4101c9",
   "metadata": {},
   "source": [
    "# Dataset importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "932bc122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components for 80% variance: 197\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Helper function to combine ECFP and SMILES datasets\n",
    "def dataset_to_df_with_smiles(ecfp_dataset, raw_dataset, tasks):\n",
    "    X_list, y_list, ids_list, smiles_list = [], [], [], []\n",
    "    \n",
    "    # Iterate through both datasets in parallel\n",
    "    for (X_batch, y_batch, w_batch, ids_batch), (X_raw, _, _, _) in zip(\n",
    "        ecfp_dataset.iterbatches(batch_size=128, pad_batches=False),\n",
    "        raw_dataset.iterbatches(batch_size=128, pad_batches=False)\n",
    "    ):\n",
    "        X_list.append(X_batch)\n",
    "        y_list.append(y_batch)\n",
    "        ids_list.extend(ids_batch)\n",
    "        smiles_list.extend(X_raw)  # SMILES strings are in the raw features\n",
    "\n",
    "    # Stack numerical and label arrays\n",
    "    X_all = np.vstack(X_list)\n",
    "    y_all = np.vstack(y_list)\n",
    "\n",
    "    # Create DataFrames\n",
    "    df_X = pd.DataFrame(X_all, columns=[f\"fp_{i}\" for i in range(X_all.shape[1])])\n",
    "    df_y = pd.DataFrame(y_all, columns=tasks)\n",
    "    df_y[\"mol_id\"] = ids_list\n",
    "    df_y[\"smiles\"] = smiles_list\n",
    "\n",
    "    # Combine all information into a single DataFrame\n",
    "    df = pd.concat([df_y, df_X], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load SIDER dataset with ECFP features (for model input)\n",
    "tasks, datasets, transformers = dc.molnet.load_sider(featurizer='ECFP', splitter='scaffold')\n",
    "train_ecfp, valid_ecfp, test_ecfp = datasets\n",
    "\n",
    "# Load SIDER dataset again with raw SMILES (for visualization / metadata)\n",
    "_, datasets_raw, _ = dc.molnet.load_sider(featurizer='Raw', splitter='scaffold')\n",
    "train_raw, valid_raw, test_raw = datasets_raw\n",
    "\n",
    "# Convert both representations to DataFrames\n",
    "df_train = dataset_to_df_with_smiles(train_ecfp, train_raw, tasks)\n",
    "df_valid = dataset_to_df_with_smiles(valid_ecfp, valid_raw, tasks)\n",
    "df_test  = dataset_to_df_with_smiles(test_ecfp,  test_raw,  tasks)\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col.startswith(\"fp_\")]\n",
    "label_cols = [col for col in df_train.columns if col not in feature_cols + ['mol_id', 'smiles', 'scaffold']]\n",
    "\n",
    "X_train = df_train[feature_cols].astype(float).values\n",
    "y_train = df_train[label_cols].astype(float).values\n",
    "\n",
    "X_valid = df_valid[feature_cols].astype(float).values\n",
    "y_valid = df_valid[label_cols].astype(float).values\n",
    "\n",
    "X_test = df_test[feature_cols].astype(float).values\n",
    "y_test = df_test[label_cols].astype(float).values\n",
    "\n",
    "pca = PCA().fit(X_train)\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Number of components to reach 80% variance\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "print(f\"Number of components for 80% variance: {n_components_80}\")\n",
    "pca = PCA(n_components=n_components_80)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_valid_pca = pca.transform(X_valid)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296c7a2",
   "metadata": {},
   "source": [
    "---\n",
    "### Baseline Modeling\n",
    "\n",
    "Now that the dataset has been explored and verified â€” including the structure, label balance, and feature sparsity â€” we can start testing basic machine learning models.\n",
    "\n",
    "The goal of this section is to establish **baseline performances** using simple algorithms (e.g., Logistic Regression, Random Forest, or simple Neural Networks) on the ECFP features.  \n",
    "These baselines will help us understand how well standard models can capture relationships between molecular fingerprints and side effects before moving to more complex or specialized architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd84aa6",
   "metadata": {},
   "source": [
    "### First wee need to evaluate our models \n",
    "\n",
    "All our models will be evaluated by the same metrics :\n",
    "\n",
    "- **Subset Accuracy**  \n",
    "  Measures the fraction of samples where *all* labels are correctly predicted.  \n",
    "  â†’ Very strict metric; typically **low** in multilabel tasks.  \n",
    "  **Good model:** > 0.5 is excellent.\n",
    "\n",
    "- **Hamming Loss**  \n",
    "  Fraction of labels that are incorrectly predicted (either 0 instead of 1 or vice versa).  \n",
    "  â†’ Lower is better.  \n",
    "  **Good model:** close to 0 means few label errors.\n",
    "\n",
    "- **Micro F1-score**  \n",
    "  Aggregates contributions of all labels to compute a global F1.  \n",
    "  â†’ Favors frequent labels; robust to imbalance.  \n",
    "  **Good model:** > 0.6â€“0.7 generally indicates solid performance.\n",
    "\n",
    "- **Macro F1-score**  \n",
    "  Averages F1 across all labels equally.  \n",
    "  â†’ Highlights performance on *rare* labels.  \n",
    "  **Good model:** similar to Micro-F1; if much lower, the model struggles on rare labels.\n",
    "\n",
    "- **Weighted F1-score**  \n",
    "  Weighted average of F1 by label frequency.  \n",
    "  â†’ Balances importance between frequent and rare labels.  \n",
    "  **Good model:** typically close to Micro-F1.\n",
    "\n",
    "- **ROC-AUC (Micro / Macro)**  \n",
    "  Measures the modelâ€™s ranking ability (probabilities).  \n",
    "  â†’ 1.0 = perfect separation, 0.5 = random guessing.  \n",
    "  **Good model:** > 0.8 for Micro-AUC, > 0.7 for Macro-AUC is usually strong.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b61d426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, hamming_loss, f1_score, roc_auc_score\n",
    ")\n",
    "\n",
    "def evaluate_multilabel_model(y_true, y_pred, y_prob=None, name=None):\n",
    "    \"\"\"\n",
    "    Evaluate multilabel classification performance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        Ground-truth binary matrix (n_samples x n_labels)\n",
    "    y_pred : np.ndarray\n",
    "        Predicted binary matrix (same shape as y_true)\n",
    "    y_prob : np.ndarray, optional\n",
    "        Predicted probabilities (for ROC-AUC if available)\n",
    "    name : str\n",
    "        Name of the dataframe evaluated\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\"Nom :\": name}\n",
    "    metrics[\"Subset accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"Hamming loss\"] = hamming_loss(y_true, y_pred)\n",
    "    metrics[\"Micro F1\"] = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    metrics[\"Macro F1\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    metrics[\"Weighted F1\"] = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    \n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            metrics[\"Micro ROC-AUC\"] = roc_auc_score(y_true, y_prob, average=\"micro\")\n",
    "            metrics[\"Macro ROC-AUC\"] = roc_auc_score(y_true, y_prob, average=\"macro\")\n",
    "        except ValueError:\n",
    "            metrics[\"Micro ROC-AUC\"] = np.nan\n",
    "            metrics[\"Macro ROC-AUC\"] = np.nan\n",
    "\n",
    "    print(\"\\nðŸ“Š Multilabel Evaluation Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"{k:20s}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"{k:20s}: {v}\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b03ba4",
   "metadata": {},
   "source": [
    "## 1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "600e02c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training baseline Logistic Regression model (One-vs-Rest)...\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Train\n",
      "Subset accuracy     : 0.6047\n",
      "Hamming loss        : 0.0361\n",
      "Micro F1            : 0.9679\n",
      "Macro F1            : 0.9551\n",
      "Weighted F1         : 0.9675\n",
      "Micro ROC-AUC       : 0.9952\n",
      "Macro ROC-AUC       : 0.9946\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Validation\n",
      "Subset accuracy     : 0.0140\n",
      "Hamming loss        : 0.2486\n",
      "Micro F1            : 0.8004\n",
      "Macro F1            : 0.6613\n",
      "Weighted F1         : 0.7947\n",
      "Micro ROC-AUC       : 0.8000\n",
      "Macro ROC-AUC       : 0.5749\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Test\n",
      "Subset accuracy     : 0.0000\n",
      "Hamming loss        : 0.2707\n",
      "Micro F1            : 0.7714\n",
      "Macro F1            : 0.6223\n",
      "Weighted F1         : 0.7726\n",
      "Micro ROC-AUC       : 0.7920\n",
      "Macro ROC-AUC       : 0.5956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Nom :': 'Test',\n",
       " 'Subset accuracy': 0.0,\n",
       " 'Hamming loss': 0.2706552706552707,\n",
       " 'Micro F1': 0.7713848173266243,\n",
       " 'Macro F1': 0.6222638991076471,\n",
       " 'Weighted F1': 0.7725664660808413,\n",
       " 'Micro ROC-AUC': 0.7920089068870102,\n",
       " 'Macro ROC-AUC': 0.5955655325627189}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# --- Train baseline model ---\n",
    "print(\"Training baseline Logistic Regression model (One-vs-Rest)...\")\n",
    "model = OneVsRestClassifier(\n",
    "    LogisticRegression(max_iter=500, solver='liblinear')\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- Predict ---\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "y_train_prob = model.predict_proba(X_train)\n",
    "y_valid_prob = model.predict_proba(X_valid)\n",
    "y_test_prob = model.predict_proba(X_test)\n",
    "\n",
    "# --- Evaluate ---\n",
    "evaluate_multilabel_model(y_train, y_train_pred, y_train_prob, \"Train\")\n",
    "evaluate_multilabel_model(y_valid, y_valid_pred, y_valid_prob, \"Validation\")\n",
    "evaluate_multilabel_model(y_test, y_test_pred, y_test_prob, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c290c0",
   "metadata": {},
   "source": [
    "### First results\n",
    "\n",
    "##### Training set\n",
    "\n",
    "- Subset accuracy: 0.6047 â†’ very high; most moleculesâ€™ labels are perfectly predicted.\n",
    "- Hamming loss: 0.0361 â†’ only 3.6% of labels are incorrect.\n",
    "- Micro / Macro / Weighted F1: â‰ˆ 0.96â€“0.97 â†’ excellent, almost perfect fitting.\n",
    "- ROC-AUC: â‰ˆ 0.995 â†’ near-perfect discrimination.\n",
    "\n",
    "Conclusion: The model fits the training data almost perfectly â€” clear overfitting symptoms.\n",
    "\n",
    "##### Validation set\n",
    "\n",
    "- Subset accuracy: 0.0140 â†’ strong drop in performance.\n",
    "- Hamming loss: 0.2486 â†’ about one quarter of labels are wrong.\n",
    "- Micro F1: 0.80 â†’ still good for frequent labels.\n",
    "- Macro F1: 0.66 â†’ poor for rare labels.\n",
    "- ROC-AUC: 0.80 (micro) / 0.57 (macro) â†’ uneven generalization.\n",
    "\n",
    "Conclusion: Large generalization gap â€” the model memorizes training patterns and struggles with unseen scaffolds.\n",
    "\n",
    "##### Test set\n",
    "\n",
    "- Subset accuracy: 0.0000 â†’ no molecule predicted perfectly.\n",
    "- Hamming loss: 0.2707 â†’ 27% of labels incorrect.\n",
    "- Micro F1: 0.77 â†’ moderate performance on common labels.\n",
    "- Macro F1: 0.62 â†’ poor performance on rare labels.\n",
    "- ROC-AUC: 0.79 (micro) / 0.60 (macro) â†’ confirms limited generalization.\n",
    "\n",
    "Conclusion: The model completely overfits to the training scaffolds and fails to generalize to new chemical structures.\n",
    "\n",
    "To address the clear overfitting observed in the SIDER scaffold split, weâ€™ll focus on improving generalization and handling label imbalance. The main goal is to help the model learn meaningful chemicalâ€“biological patterns rather than memorizing scaffolds.\n",
    "\n",
    "**Possible solutions:**\n",
    "\n",
    "- Increase regularization â†’ reduces model complexity and prevents memorization of training data.\n",
    "- Apply dimensionality reduction (e.g., PCA, SVD) â†’ lowers feature redundancy in high-dimensional fingerprints.\n",
    "- Use class weighting or resampling â†’ balances frequent and rare side-effect labels.\n",
    "- Ensure strict scaffold separation â†’ avoids any structural leakage between splits.\n",
    "- Try more robust models (e.g., Random Forest, XGBoost) â†’ better capture non-linear relationships without overfitting as easily.\n",
    "\n",
    "These adjustments aim to improve the modelâ€™s ability to generalize across unseen molecular scaffolds while maintaining balanced predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43de33b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Logistic Regression on PCA-reduced Data ===\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Train (PCA)\n",
      "Subset accuracy     : 0.0272\n",
      "Hamming loss        : 0.2209\n",
      "Micro F1            : 0.7933\n",
      "Macro F1            : 0.7567\n",
      "Weighted F1         : 0.8011\n",
      "Micro ROC-AUC       : 0.8666\n",
      "Macro ROC-AUC       : 0.8638\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Validation (PCA)\n",
      "Subset accuracy     : 0.0000\n",
      "Hamming loss        : 0.3025\n",
      "Micro F1            : 0.7537\n",
      "Macro F1            : 0.6449\n",
      "Weighted F1         : 0.7649\n",
      "Micro ROC-AUC       : 0.7579\n",
      "Macro ROC-AUC       : 0.5694\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Test (PCA)\n",
      "Subset accuracy     : 0.0070\n",
      "Hamming loss        : 0.3217\n",
      "Micro F1            : 0.7244\n",
      "Macro F1            : 0.6078\n",
      "Weighted F1         : 0.7434\n",
      "Micro ROC-AUC       : 0.7456\n",
      "Macro ROC-AUC       : 0.5862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Nom :': 'Test (PCA)',\n",
       " 'Subset accuracy': 0.006993006993006993,\n",
       " 'Hamming loss': 0.32167832167832167,\n",
       " 'Micro F1': 0.7243675099866844,\n",
       " 'Macro F1': 0.6077583706053509,\n",
       " 'Weighted F1': 0.7434387494481324,\n",
       " 'Micro ROC-AUC': 0.7456005872432856,\n",
       " 'Macro ROC-AUC': 0.5861874940246806}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pca = OneVsRestClassifier(\n",
    "    LogisticRegression(max_iter=500, solver='liblinear', class_weight='balanced')\n",
    ")\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "y_train_pred_pca = model_pca.predict(X_train_pca)\n",
    "y_valid_pred_pca = model_pca.predict(X_valid_pca)\n",
    "y_test_pred_pca = model_pca.predict(X_test_pca)\n",
    "\n",
    "y_train_prob_pca = model_pca.predict_proba(X_train_pca)\n",
    "y_valid_prob_pca = model_pca.predict_proba(X_valid_pca)\n",
    "y_test_prob_pca = model_pca.predict_proba(X_test_pca)\n",
    "\n",
    "print(\"=== Logistic Regression on PCA-reduced Data ===\")\n",
    "evaluate_multilabel_model(y_train, y_train_pred_pca, y_train_prob_pca, \"Train (PCA)\")\n",
    "evaluate_multilabel_model(y_valid, y_valid_pred_pca, y_valid_prob_pca, \"Validation (PCA)\")\n",
    "evaluate_multilabel_model(y_test, y_test_pred_pca, y_test_prob_pca, \"Test (PCA)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c56148c",
   "metadata": {},
   "source": [
    "Our logistic regression with PCA traininig datasets is just slightly better than our previous model. It still overfits. We need to try with another model to evaluate the revelance of our PCA. We can not use logistical regression to classify our dataset.\n",
    "\n",
    "If all our models overfit, we will need to get back on our datas and retry oour logiistic regression but for now, let's try Random Forest in the 3rd notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4a86271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ EntraÃ®nement de la RÃ©gression Logistique L1 (Lasso)...\n",
      "=== Logistic Regression on Lasso L1 penalty ===\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Train (PCA)\n",
      "Subset accuracy     : 0.0044\n",
      "Hamming loss        : 0.3308\n",
      "Micro F1            : 0.6925\n",
      "Macro F1            : 0.6389\n",
      "Weighted F1         : 0.7095\n",
      "Micro ROC-AUC       : 0.7382\n",
      "Macro ROC-AUC       : 0.7322\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Validation (PCA)\n",
      "Subset accuracy     : 0.0000\n",
      "Hamming loss        : 0.3341\n",
      "Micro F1            : 0.7454\n",
      "Macro F1            : 0.6581\n",
      "Weighted F1         : 0.7744\n",
      "Micro ROC-AUC       : 0.6867\n",
      "Macro ROC-AUC       : 0.5449\n",
      "\n",
      "ðŸ“Š Multilabel Evaluation Metrics:\n",
      "Nom :               : Test (PCA)\n",
      "Subset accuracy     : 0.0000\n",
      "Hamming loss        : 0.3592\n",
      "Micro F1            : 0.7111\n",
      "Macro F1            : 0.6245\n",
      "Weighted F1         : 0.7528\n",
      "Micro ROC-AUC       : 0.6981\n",
      "Macro ROC-AUC       : 0.6157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Nom :': 'Test (PCA)',\n",
       " 'Subset accuracy': 0.0,\n",
       " 'Hamming loss': 0.35923335923335925,\n",
       " 'Micro F1': 0.7111018537804624,\n",
       " 'Macro F1': 0.6245410080397917,\n",
       " 'Weighted F1': 0.7527603122901252,\n",
       " 'Micro ROC-AUC': 0.69808913981957,\n",
       " 'Macro ROC-AUC': 0.6156967889007495}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- EntraÃ®ner la RÃ©gression Logistique L1 ---\n",
    "print(\"ðŸš€ EntraÃ®nement de la RÃ©gression Logistique L1 (Lasso)...\")\n",
    "# C=0.1 correspond Ã  une rÃ©gularisation L1 assez forte\n",
    "l1_model = OneVsRestClassifier(\n",
    "    LogisticRegression(penalty='l1', C=0.1, solver='liblinear', class_weight='balanced', max_iter=1000)\n",
    ")\n",
    "l1_model.fit(X_train, y_train)\n",
    "\n",
    "# --- PrÃ©dictions ---\n",
    "y_train_pred_l1 = l1_model.predict(X_train)\n",
    "y_valid_pred_l1 = l1_model.predict(X_valid)\n",
    "y_test_pred_l1 = l1_model.predict(X_test)\n",
    "\n",
    "y_train_prob_l1 = l1_model.predict_proba(X_train)\n",
    "y_valid_prob_l1 = l1_model.predict_proba(X_valid)\n",
    "y_test_prob_l1 = l1_model.predict_proba(X_test)\n",
    "\n",
    "print(\"=== Logistic Regression on Lasso L1 penalty ===\")\n",
    "evaluate_multilabel_model(y_train, y_train_pred_l1, y_train_prob_l1, \"Train (PCA)\")\n",
    "evaluate_multilabel_model(y_valid, y_valid_pred_l1, y_valid_prob_l1, \"Validation (PCA)\")\n",
    "evaluate_multilabel_model(y_test, y_test_pred_l1, y_test_prob_l1, \"Test (PCA)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724723f4",
   "metadata": {},
   "source": [
    "Unlike the previous models, the L1-regularized Logistic Regression shows significantly lower performance on the training set. This is a strong indication that the L1 penalty is effectively reducing overfitting. The model is no longer memorizing the training data. The scores are much more modest, suggesting a simpler model constrained by the regularization.\n",
    "\n",
    "The validation performance is surprisingly similar to or even slightly better than the training performance in some metrics (F1 scores), but worse in others (AUC). The gap between training and validation is almost eliminated, confirming that overfitting is significantly reduced. However, the overall performance, especially the Macro ROC-AUC (0.54), is quite poor. The strong regularization might be causing underfitting or filtering out too many relevant features.\n",
    "\n",
    "The test performance is generally consistent with the validation set, confirming good generalization (i.e., minimal overfitting). However, the absolute performance level remains low, especially concerning the Macro F1 and Macro ROC-AUC scores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cheminformatics_project)",
   "language": "python",
   "name": "cheminformatics_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

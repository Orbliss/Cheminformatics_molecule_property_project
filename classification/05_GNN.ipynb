{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360197bd-0eeb-49dc-b7e2-3279147da243",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score, hamming_loss, f1_score, roc_auc_score\n",
    ")\n",
    "def evaluate_multilabel_model(y_true, y_pred, y_prob=None, name=None):\n",
    "    \"\"\"\n",
    "    Evaluate multilabel classification performance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : np.ndarray\n",
    "        Ground-truth binary matrix (n_samples x n_labels)\n",
    "    y_pred : np.ndarray\n",
    "        Predicted binary matrix (same shape as y_true)\n",
    "    y_prob : np.ndarray, optional\n",
    "        Predicted probabilities (for ROC-AUC if available)\n",
    "    name : str\n",
    "        Name of the dataframe evaluated\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = {\"Nom :\": name}\n",
    "    metrics[\"Subset accuracy\"] = accuracy_score(y_true, y_pred)\n",
    "    metrics[\"Hamming loss\"] = hamming_loss(y_true, y_pred)\n",
    "    metrics[\"Micro F1\"] = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    metrics[\"Macro F1\"] = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    metrics[\"Weighted F1\"] = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "    \n",
    "    if y_prob is not None:\n",
    "        try:\n",
    "            metrics[\"Micro ROC-AUC\"] = roc_auc_score(y_true, y_prob, average=\"micro\")\n",
    "            metrics[\"Macro ROC-AUC\"] = roc_auc_score(y_true, y_prob, average=\"macro\")\n",
    "        except ValueError:\n",
    "            metrics[\"Micro ROC-AUC\"] = np.nan\n",
    "            metrics[\"Macro ROC-AUC\"] = np.nan\n",
    "\n",
    "    print(\"\\nüìä Multilabel Evaluation Metrics:\")\n",
    "    for k, v in metrics.items():\n",
    "        if isinstance(v, float):\n",
    "            print(f\"{k:20s}: {v:.4f}\")\n",
    "        else:\n",
    "            print(f\"{k:20s}: {v}\")\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eefa90b0-fa36-457f-8c4a-4660deb81c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No normalization for SPS. Feature removed!\n",
      "No normalization for AvgIpc. Feature removed!\n",
      "No normalization for NumAmideBonds. Feature removed!\n",
      "No normalization for NumAtomStereoCenters. Feature removed!\n",
      "No normalization for NumBridgeheadAtoms. Feature removed!\n",
      "No normalization for NumHeterocycles. Feature removed!\n",
      "No normalization for NumSpiroAtoms. Feature removed!\n",
      "No normalization for NumUnspecifiedAtomStereoCenters. Feature removed!\n",
      "No normalization for Phi. Feature removed!\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow.python'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torchdata.datapipes'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'haiku'\n",
      "Skipped loading some PyTorch models, missing a dependency. No module named 'tensorflow.python'\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Helper function to combine ECFP and SMILES datasets\n",
    "def dataset_to_df_with_smiles(ecfp_dataset, raw_dataset, tasks):\n",
    "    X_list, y_list, ids_list, smiles_list = [], [], [], []\n",
    "    \n",
    "    # Iterate through both datasets in parallel\n",
    "    for (X_batch, y_batch, w_batch, ids_batch), (X_raw, _, _, _) in zip(\n",
    "        ecfp_dataset.iterbatches(batch_size=128, pad_batches=False),\n",
    "        raw_dataset.iterbatches(batch_size=128, pad_batches=False)\n",
    "    ):\n",
    "        X_list.append(X_batch)\n",
    "        y_list.append(y_batch)\n",
    "        ids_list.extend(ids_batch)\n",
    "        smiles_list.extend(X_raw)  # SMILES strings are in the raw features\n",
    "\n",
    "    # Stack numerical and label arrays\n",
    "    X_all = np.vstack(X_list)\n",
    "    y_all = np.vstack(y_list)\n",
    "\n",
    "    # Create DataFrames\n",
    "    df_X = pd.DataFrame(X_all, columns=[f\"fp_{i}\" for i in range(X_all.shape[1])])\n",
    "    df_y = pd.DataFrame(y_all, columns=tasks)\n",
    "    df_y[\"mol_id\"] = ids_list\n",
    "    df_y[\"smiles\"] = smiles_list\n",
    "\n",
    "    # Combine all information into a single DataFrame\n",
    "    df = pd.concat([df_y, df_X], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load SIDER dataset with ECFP features (for model input)\n",
    "tasks, datasets, transformers = dc.molnet.load_sider(featurizer='ECFP', splitter='scaffold')\n",
    "train_ecfp, valid_ecfp, test_ecfp = datasets\n",
    "\n",
    "# Load SIDER dataset again with raw SMILES (for visualization / metadata)\n",
    "_, datasets_raw, _ = dc.molnet.load_sider(featurizer='Raw', splitter='scaffold')\n",
    "train_raw, valid_raw, test_raw = datasets_raw\n",
    "\n",
    "# Convert both representations to DataFrames\n",
    "df_train = dataset_to_df_with_smiles(train_ecfp, train_raw, tasks)\n",
    "df_valid = dataset_to_df_with_smiles(valid_ecfp, valid_raw, tasks)\n",
    "df_test  = dataset_to_df_with_smiles(test_ecfp,  test_raw,  tasks)\n",
    "\n",
    "feature_cols = [col for col in df_train.columns if col.startswith(\"fp_\")]\n",
    "label_cols = [col for col in df_train.columns if col not in feature_cols + ['mol_id', 'smiles', 'scaffold']]\n",
    "\n",
    "X_train = df_train[feature_cols].astype(float).values\n",
    "y_train = df_train[label_cols].astype(float).values\n",
    "\n",
    "X_valid = df_valid[feature_cols].astype(float).values\n",
    "y_valid = df_valid[label_cols].astype(float).values\n",
    "\n",
    "X_test = df_test[feature_cols].astype(float).values\n",
    "y_test = df_test[label_cols].astype(float).values\n",
    "\n",
    "pca = PCA().fit(X_train)\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Number of components to reach 80% variance\n",
    "n_components_80 = np.argmax(cumulative_variance >= 0.80) + 1\n",
    "\n",
    "pca = PCA(n_components=n_components_80)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_valid_pca = pca.transform(X_valid)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d039ab-6ba4-465c-8070-fbf4878a6c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de t√¢ches : 27\n",
      "Train samples : 1141\n",
      "Valid samples : 143\n",
      "Test samples  : 143\n"
     ]
    }
   ],
   "source": [
    "# Charger le dataset SIDER avec un featurizer de graphes\n",
    "tasks, datasets, transformers = dc.molnet.load_sider(\n",
    "    featurizer=dc.feat.ConvMolFeaturizer(), \n",
    "    splitter='scaffold'\n",
    ")\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "\n",
    "print(f\"Nombre de t√¢ches : {len(tasks)}\")\n",
    "print(f\"Train samples : {len(train_dataset)}\")\n",
    "print(f\"Valid samples : {len(valid_dataset)}\")\n",
    "print(f\"Test samples  : {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "decf8cb2-23c9-49a7-b795-272abf879a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üè∑Ô∏è Retrieving ground truth labels (y_true) from original DataFrames...\n",
      "‚úÖ Ground truth label arrays created (y_train shape: (1141, 27))\n"
     ]
    }
   ],
   "source": [
    "# --- Retrieve Ground Truth Labels (y_true) ---\n",
    "print(\"üè∑Ô∏è Retrieving ground truth labels (y_true) from original DataFrames...\")\n",
    "# Check if label_cols exists, otherwise define it (robustness)\n",
    "if 'label_cols' not in locals():\n",
    "    print(\"   'label_cols' not found, defining it now from df_train...\")\n",
    "    label_cols = [col for col in df_train.columns if col not in ['mol_id', 'smiles', 'scaffold'] and not col.startswith(\"fp_\")]\n",
    "    if not label_cols:\n",
    "        raise ValueError(\"Could not define label_cols. Make sure df_train is loaded and has label columns.\")\n",
    "\n",
    "y_train = df_train[label_cols].astype(float).values\n",
    "y_valid = df_valid[label_cols].astype(float).values\n",
    "y_test = df_test[label_cols].astype(float).values\n",
    "print(f\"‚úÖ Ground truth label arrays created (y_train shape: {y_train.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e6e874-8933-4042-9b13-7747f7f7b5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from rdkit import RDLogger\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "lg = RDLogger.logger()\n",
    "lg.setLevel(RDLogger.CRITICAL) # This tells RDKit's own logger to only show critical errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a762726-0557-42de-9695-ebc6544179af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Loading SIDER dataset using MoleculeNetDataset...\n",
      "‚úÖ SIDER raw datasets loaded.\n",
      "üß¨ Creating PyTorch Geometric datasets...\n",
      "‚úÖ PyG Datasets created.\n",
      "‚úÖ PyG DataLoaders created.\n",
      "üè∑Ô∏è Retrieving ground truth labels (y_true) for evaluation...\n",
      "‚úÖ Ground truth label arrays created (y_train_true shape: (1141, 27))\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "import torch\n",
    "from torch_geometric.data import Data, Batch # PyTorch Geometric Data format\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- 1. Load SIDER dataset using MoleculeNetDataset ---\n",
    "print(\"üîÑ Loading SIDER dataset using MoleculeNetDataset...\")\n",
    "sider_tasks, (train_raw_ds, valid_raw_ds, test_raw_ds), transformers = dc.molnet.load_sider(\n",
    "    featurizer=dc.feat.DummyFeaturizer(), # Load raw SMILES first\n",
    "    splitter='scaffold',\n",
    "    transformers=[] # No initial transformers\n",
    ")\n",
    "print(\"‚úÖ SIDER raw datasets loaded.\")\n",
    "\n",
    "# --- 2. Define Featurizer for PyG (Manual Feature Extraction) ---\n",
    "# We'll extract atom features and adjacency info manually for PyG's format\n",
    "\n",
    "def smiles_to_pyg_graph(smiles, y):\n",
    "    \"\"\"Converts SMILES string to PyTorch Geometric Data object.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    # --- Atom Features ---\n",
    "    atom_features = []\n",
    "    allowable_features = {\n",
    "        'possible_atomic_num_list': list(range(1, 119)),\n",
    "        'possible_formal_charge_list': [-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5],\n",
    "        'possible_chirality_list': [\n",
    "            Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER\n",
    "        ],\n",
    "        'possible_hybridization_list': [\n",
    "            Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "            Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2,\n",
    "            Chem.rdchem.HybridizationType.UNSPECIFIED\n",
    "        ],\n",
    "        'possible_numH_list': [0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'possible_implicit_valence_list': [0, 1, 2, 3, 4, 5, 6],\n",
    "        'possible_degree_list': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "        'possible_bonds': [\n",
    "            Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "            Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for atom in mol.GetAtoms():\n",
    "        features = []\n",
    "        features.append(allowable_features['possible_atomic_num_list'].index(atom.GetAtomicNum()))\n",
    "        features.append(allowable_features['possible_formal_charge_list'].index(atom.GetFormalCharge()))\n",
    "        features.append(allowable_features['possible_chirality_list'].index(atom.GetChiralTag()))\n",
    "        features.append(allowable_features['possible_hybridization_list'].index(atom.GetHybridization()))\n",
    "        features.append(allowable_features['possible_numH_list'].index(atom.GetTotalNumHs()))\n",
    "        features.append(allowable_features['possible_implicit_valence_list'].index(atom.GetImplicitValence()))\n",
    "        features.append(allowable_features['possible_degree_list'].index(atom.GetDegree()))\n",
    "        features.append(atom.GetIsAromatic())\n",
    "        features.append(atom.IsInRing())\n",
    "        atom_features.append(features)\n",
    "    x = torch.tensor(np.array(atom_features), dtype=torch.float)\n",
    "\n",
    "    # --- Edge Index (Connectivity) ---\n",
    "    edge_indices = []\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        edge_indices += [[i, j], [j, i]] # Add edges in both directions\n",
    "    edge_index = torch.tensor(np.array(edge_indices).T, dtype=torch.long)\n",
    "\n",
    "    # --- Labels ---\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float).unsqueeze(0) # Shape [1, n_tasks]\n",
    "\n",
    "    # Create PyG Data object\n",
    "    data = Data(x=x, edge_index=edge_index, y=y_tensor)\n",
    "    return data\n",
    "\n",
    "# --- 3. Create PyTorch Geometric Datasets ---\n",
    "class SIDERPyGDataset(Dataset):\n",
    "    def __init__(self, dc_dataset):\n",
    "        self.smiles = dc_dataset.ids\n",
    "        self.labels = dc_dataset.y\n",
    "        self.graphs = []\n",
    "        for i in range(len(self.smiles)):\n",
    "            graph = smiles_to_pyg_graph(self.smiles[i], self.labels[i])\n",
    "            if graph is not None:\n",
    "                self.graphs.append(graph)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "print(\"üß¨ Creating PyTorch Geometric datasets...\")\n",
    "train_pyg_dataset = SIDERPyGDataset(train_raw_ds)\n",
    "valid_pyg_dataset = SIDERPyGDataset(valid_raw_ds)\n",
    "test_pyg_dataset = SIDERPyGDataset(test_raw_ds)\n",
    "print(\"‚úÖ PyG Datasets created.\")\n",
    "\n",
    "# --- 4. Create PyTorch DataLoaders ---\n",
    "# Use PyG's DataLoader for handling batches of graphs\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "\n",
    "batch_size = 64 # Adjust as needed\n",
    "train_loader = PyGDataLoader(train_pyg_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = PyGDataLoader(valid_pyg_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = PyGDataLoader(test_pyg_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"‚úÖ PyG DataLoaders created.\")\n",
    "\n",
    "\n",
    "# --- 5. Retrieve y_true again (needed for evaluation) ---\n",
    "print(\"üè∑Ô∏è Retrieving ground truth labels (y_true) for evaluation...\")\n",
    "if 'label_cols' not in locals():\n",
    "    label_cols = sider_tasks # Use tasks list from loader\n",
    "y_train_list = [data.y.numpy().flatten() for data in train_pyg_dataset]\n",
    "y_valid_list = [data.y.numpy().flatten() for data in valid_pyg_dataset]\n",
    "y_test_list = [data.y.numpy().flatten() for data in test_pyg_dataset]\n",
    "\n",
    "y_train_true = np.array(y_train_list)\n",
    "y_valid_true = np.array(y_valid_list)\n",
    "y_test_true = np.array(y_test_list)\n",
    "print(f\"‚úÖ Ground truth label arrays created (y_train_true shape: {y_train_true.shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae1042ee-243f-4698-9a79-975eab5a87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "‚úÖ PyTorch Geometric GNN model instantiated.\n",
      "üöÄ Training PyTorch Geometric GNN for 200 epochs...\n",
      "Epoch 01, Train Loss: 0.5872, Valid Loss: 0.4824\n",
      "Epoch 02, Train Loss: 0.5352, Valid Loss: 0.4835\n",
      "Epoch 03, Train Loss: 0.5259, Valid Loss: 0.4792\n",
      "Epoch 04, Train Loss: 0.5236, Valid Loss: 0.4798\n",
      "Epoch 05, Train Loss: 0.5206, Valid Loss: 0.4862\n",
      "Epoch 06, Train Loss: 0.5231, Valid Loss: 0.4832\n",
      "Epoch 07, Train Loss: 0.5221, Valid Loss: 0.4768\n",
      "Epoch 08, Train Loss: 0.5201, Valid Loss: 0.4801\n",
      "Epoch 09, Train Loss: 0.5198, Valid Loss: 0.4817\n",
      "Epoch 10, Train Loss: 0.5182, Valid Loss: 0.4769\n",
      "Epoch 11, Train Loss: 0.5183, Valid Loss: 0.4789\n",
      "Epoch 12, Train Loss: 0.5193, Valid Loss: 0.4869\n",
      "Epoch 13, Train Loss: 0.5187, Valid Loss: 0.4787\n",
      "Epoch 14, Train Loss: 0.5180, Valid Loss: 0.4815\n",
      "Epoch 15, Train Loss: 0.5160, Valid Loss: 0.4806\n",
      "Epoch 16, Train Loss: 0.5158, Valid Loss: 0.4861\n",
      "Epoch 17, Train Loss: 0.5166, Valid Loss: 0.4884\n",
      "Epoch 18, Train Loss: 0.5156, Valid Loss: 0.4750\n",
      "Epoch 19, Train Loss: 0.5158, Valid Loss: 0.4741\n",
      "Epoch 20, Train Loss: 0.5149, Valid Loss: 0.4760\n",
      "Epoch 21, Train Loss: 0.5138, Valid Loss: 0.4779\n",
      "Epoch 22, Train Loss: 0.5145, Valid Loss: 0.4759\n",
      "Epoch 23, Train Loss: 0.5132, Valid Loss: 0.4756\n",
      "Epoch 24, Train Loss: 0.5136, Valid Loss: 0.4844\n",
      "Epoch 25, Train Loss: 0.5149, Valid Loss: 0.4766\n",
      "Epoch 26, Train Loss: 0.5117, Valid Loss: 0.4749\n",
      "Epoch 27, Train Loss: 0.5137, Valid Loss: 0.4781\n",
      "Epoch 28, Train Loss: 0.5122, Valid Loss: 0.4765\n",
      "Epoch 29, Train Loss: 0.5122, Valid Loss: 0.4769\n",
      "Epoch 30, Train Loss: 0.5107, Valid Loss: 0.4751\n",
      "Epoch 31, Train Loss: 0.5117, Valid Loss: 0.4755\n",
      "Epoch 32, Train Loss: 0.5096, Valid Loss: 0.4756\n",
      "Epoch 33, Train Loss: 0.5096, Valid Loss: 0.4752\n",
      "Epoch 34, Train Loss: 0.5120, Valid Loss: 0.4769\n",
      "Epoch 35, Train Loss: 0.5094, Valid Loss: 0.4757\n",
      "Epoch 36, Train Loss: 0.5100, Valid Loss: 0.4741\n",
      "Epoch 37, Train Loss: 0.5091, Valid Loss: 0.4747\n",
      "Epoch 38, Train Loss: 0.5079, Valid Loss: 0.4764\n",
      "Epoch 39, Train Loss: 0.5086, Valid Loss: 0.4744\n",
      "Epoch 40, Train Loss: 0.5073, Valid Loss: 0.4739\n",
      "Epoch 41, Train Loss: 0.5090, Valid Loss: 0.4746\n",
      "Epoch 42, Train Loss: 0.5064, Valid Loss: 0.4719\n",
      "Epoch 43, Train Loss: 0.5072, Valid Loss: 0.4738\n",
      "Epoch 44, Train Loss: 0.5051, Valid Loss: 0.4795\n",
      "Epoch 45, Train Loss: 0.5056, Valid Loss: 0.4793\n",
      "Epoch 46, Train Loss: 0.5049, Valid Loss: 0.4768\n",
      "Epoch 47, Train Loss: 0.5048, Valid Loss: 0.4748\n",
      "Epoch 48, Train Loss: 0.5038, Valid Loss: 0.4753\n",
      "Epoch 49, Train Loss: 0.5033, Valid Loss: 0.4822\n",
      "Epoch 50, Train Loss: 0.5035, Valid Loss: 0.4755\n",
      "Epoch 51, Train Loss: 0.5034, Valid Loss: 0.4752\n",
      "Epoch 52, Train Loss: 0.5026, Valid Loss: 0.4723\n",
      "Epoch 53, Train Loss: 0.5030, Valid Loss: 0.4714\n",
      "Epoch 54, Train Loss: 0.5037, Valid Loss: 0.4793\n",
      "Epoch 55, Train Loss: 0.5035, Valid Loss: 0.4760\n",
      "Epoch 56, Train Loss: 0.5034, Valid Loss: 0.4825\n",
      "Epoch 57, Train Loss: 0.5007, Valid Loss: 0.4750\n",
      "Epoch 58, Train Loss: 0.5015, Valid Loss: 0.4803\n",
      "Epoch 59, Train Loss: 0.5009, Valid Loss: 0.4755\n",
      "Epoch 60, Train Loss: 0.5010, Valid Loss: 0.4727\n",
      "Epoch 61, Train Loss: 0.4988, Valid Loss: 0.4748\n",
      "Epoch 62, Train Loss: 0.4994, Valid Loss: 0.4761\n",
      "Epoch 63, Train Loss: 0.5000, Valid Loss: 0.4724\n",
      "Epoch 64, Train Loss: 0.5009, Valid Loss: 0.4746\n",
      "Epoch 65, Train Loss: 0.5013, Valid Loss: 0.4733\n",
      "Epoch 66, Train Loss: 0.4994, Valid Loss: 0.4739\n",
      "Epoch 67, Train Loss: 0.4976, Valid Loss: 0.4724\n",
      "Epoch 68, Train Loss: 0.4979, Valid Loss: 0.4740\n",
      "Epoch 69, Train Loss: 0.4966, Valid Loss: 0.4729\n",
      "Epoch 70, Train Loss: 0.4943, Valid Loss: 0.4736\n",
      "Epoch 71, Train Loss: 0.4949, Valid Loss: 0.4723\n",
      "Epoch 72, Train Loss: 0.4975, Valid Loss: 0.4715\n",
      "Epoch 73, Train Loss: 0.4973, Valid Loss: 0.4716\n",
      "Epoch 74, Train Loss: 0.4965, Valid Loss: 0.4745\n",
      "Epoch 75, Train Loss: 0.4956, Valid Loss: 0.4726\n",
      "Epoch 76, Train Loss: 0.4948, Valid Loss: 0.4719\n",
      "Epoch 77, Train Loss: 0.4963, Valid Loss: 0.4751\n",
      "Epoch 78, Train Loss: 0.4943, Valid Loss: 0.4730\n",
      "Epoch 79, Train Loss: 0.4938, Valid Loss: 0.4729\n",
      "Epoch 80, Train Loss: 0.4916, Valid Loss: 0.4783\n",
      "Epoch 81, Train Loss: 0.4949, Valid Loss: 0.4777\n",
      "Epoch 82, Train Loss: 0.4949, Valid Loss: 0.4788\n",
      "Epoch 83, Train Loss: 0.4985, Valid Loss: 0.4800\n",
      "Epoch 84, Train Loss: 0.4967, Valid Loss: 0.4763\n",
      "Epoch 85, Train Loss: 0.4920, Valid Loss: 0.4743\n",
      "Epoch 86, Train Loss: 0.4909, Valid Loss: 0.4746\n",
      "Epoch 87, Train Loss: 0.4920, Valid Loss: 0.4729\n",
      "Epoch 88, Train Loss: 0.4910, Valid Loss: 0.4742\n",
      "Epoch 89, Train Loss: 0.4918, Valid Loss: 0.4747\n",
      "Epoch 90, Train Loss: 0.4914, Valid Loss: 0.4731\n",
      "Epoch 91, Train Loss: 0.4895, Valid Loss: 0.4776\n",
      "Epoch 92, Train Loss: 0.4896, Valid Loss: 0.4754\n",
      "Epoch 93, Train Loss: 0.4899, Valid Loss: 0.4730\n",
      "Epoch 94, Train Loss: 0.4877, Valid Loss: 0.4773\n",
      "Epoch 95, Train Loss: 0.4898, Valid Loss: 0.4747\n",
      "Epoch 96, Train Loss: 0.4873, Valid Loss: 0.4775\n",
      "Epoch 97, Train Loss: 0.4900, Valid Loss: 0.4799\n",
      "Epoch 98, Train Loss: 0.4907, Valid Loss: 0.4752\n",
      "Epoch 99, Train Loss: 0.4893, Valid Loss: 0.4792\n",
      "Epoch 100, Train Loss: 0.4867, Valid Loss: 0.4742\n",
      "Epoch 101, Train Loss: 0.4885, Valid Loss: 0.4727\n",
      "Epoch 102, Train Loss: 0.4882, Valid Loss: 0.4793\n",
      "Epoch 103, Train Loss: 0.4923, Valid Loss: 0.4764\n",
      "Epoch 104, Train Loss: 0.4904, Valid Loss: 0.4795\n",
      "Epoch 105, Train Loss: 0.4862, Valid Loss: 0.4767\n",
      "Epoch 106, Train Loss: 0.4851, Valid Loss: 0.4739\n",
      "Epoch 107, Train Loss: 0.4870, Valid Loss: 0.4776\n",
      "Epoch 108, Train Loss: 0.4845, Valid Loss: 0.4718\n",
      "Epoch 109, Train Loss: 0.4864, Valid Loss: 0.4728\n",
      "Epoch 110, Train Loss: 0.4880, Valid Loss: 0.4753\n",
      "Epoch 111, Train Loss: 0.4876, Valid Loss: 0.4737\n",
      "Epoch 112, Train Loss: 0.4853, Valid Loss: 0.4726\n",
      "Epoch 113, Train Loss: 0.4816, Valid Loss: 0.4724\n",
      "Epoch 114, Train Loss: 0.4817, Valid Loss: 0.4723\n",
      "Epoch 115, Train Loss: 0.4828, Valid Loss: 0.4882\n",
      "Epoch 116, Train Loss: 0.4843, Valid Loss: 0.4733\n",
      "Epoch 117, Train Loss: 0.4826, Valid Loss: 0.4806\n",
      "Epoch 118, Train Loss: 0.4865, Valid Loss: 0.4748\n",
      "Epoch 119, Train Loss: 0.4826, Valid Loss: 0.4743\n",
      "Epoch 120, Train Loss: 0.4831, Valid Loss: 0.4740\n",
      "Epoch 121, Train Loss: 0.4804, Valid Loss: 0.4724\n",
      "Epoch 122, Train Loss: 0.4803, Valid Loss: 0.4720\n",
      "Epoch 123, Train Loss: 0.4831, Valid Loss: 0.4762\n",
      "Epoch 124, Train Loss: 0.4814, Valid Loss: 0.4740\n",
      "Epoch 125, Train Loss: 0.4829, Valid Loss: 0.4741\n",
      "Epoch 126, Train Loss: 0.4790, Valid Loss: 0.4734\n",
      "Epoch 127, Train Loss: 0.4799, Valid Loss: 0.4746\n",
      "Epoch 128, Train Loss: 0.4787, Valid Loss: 0.4736\n",
      "Epoch 129, Train Loss: 0.4811, Valid Loss: 0.4732\n",
      "Epoch 130, Train Loss: 0.4793, Valid Loss: 0.4770\n",
      "Epoch 131, Train Loss: 0.4804, Valid Loss: 0.4744\n",
      "Epoch 132, Train Loss: 0.4792, Valid Loss: 0.4752\n",
      "Epoch 133, Train Loss: 0.4805, Valid Loss: 0.4746\n",
      "Epoch 134, Train Loss: 0.4776, Valid Loss: 0.4766\n",
      "Epoch 135, Train Loss: 0.4773, Valid Loss: 0.4740\n",
      "Epoch 136, Train Loss: 0.4756, Valid Loss: 0.4743\n",
      "Epoch 137, Train Loss: 0.4807, Valid Loss: 0.4755\n",
      "Epoch 138, Train Loss: 0.4803, Valid Loss: 0.4775\n",
      "Epoch 139, Train Loss: 0.4794, Valid Loss: 0.4755\n",
      "Epoch 140, Train Loss: 0.4780, Valid Loss: 0.4743\n",
      "Epoch 141, Train Loss: 0.4780, Valid Loss: 0.4724\n",
      "Epoch 142, Train Loss: 0.4764, Valid Loss: 0.4736\n",
      "Epoch 143, Train Loss: 0.4758, Valid Loss: 0.4747\n",
      "Epoch 144, Train Loss: 0.4737, Valid Loss: 0.4766\n",
      "Epoch 145, Train Loss: 0.4759, Valid Loss: 0.4721\n",
      "Epoch 146, Train Loss: 0.4762, Valid Loss: 0.4742\n",
      "Epoch 147, Train Loss: 0.4770, Valid Loss: 0.4739\n",
      "Epoch 148, Train Loss: 0.4756, Valid Loss: 0.4778\n",
      "Epoch 149, Train Loss: 0.4762, Valid Loss: 0.4817\n",
      "Epoch 150, Train Loss: 0.4780, Valid Loss: 0.4719\n",
      "Epoch 151, Train Loss: 0.4834, Valid Loss: 0.4715\n",
      "Epoch 152, Train Loss: 0.4763, Valid Loss: 0.4737\n",
      "Epoch 153, Train Loss: 0.4723, Valid Loss: 0.4722\n",
      "Epoch 154, Train Loss: 0.4749, Valid Loss: 0.4753\n",
      "Epoch 155, Train Loss: 0.4732, Valid Loss: 0.4744\n",
      "Epoch 156, Train Loss: 0.4720, Valid Loss: 0.4776\n",
      "Epoch 157, Train Loss: 0.4744, Valid Loss: 0.4814\n",
      "Epoch 158, Train Loss: 0.4757, Valid Loss: 0.4745\n",
      "Epoch 159, Train Loss: 0.4710, Valid Loss: 0.4721\n",
      "Epoch 160, Train Loss: 0.4740, Valid Loss: 0.4745\n",
      "Epoch 161, Train Loss: 0.4723, Valid Loss: 0.4745\n",
      "Epoch 162, Train Loss: 0.4749, Valid Loss: 0.4751\n",
      "Epoch 163, Train Loss: 0.4740, Valid Loss: 0.4729\n",
      "Epoch 164, Train Loss: 0.4720, Valid Loss: 0.4799\n",
      "Epoch 165, Train Loss: 0.4714, Valid Loss: 0.4775\n",
      "Epoch 166, Train Loss: 0.4714, Valid Loss: 0.4737\n",
      "Epoch 167, Train Loss: 0.4709, Valid Loss: 0.4739\n",
      "Epoch 168, Train Loss: 0.4708, Valid Loss: 0.4762\n",
      "Epoch 169, Train Loss: 0.4729, Valid Loss: 0.4761\n",
      "Epoch 170, Train Loss: 0.4748, Valid Loss: 0.4741\n",
      "Epoch 171, Train Loss: 0.4716, Valid Loss: 0.4733\n",
      "Epoch 172, Train Loss: 0.4683, Valid Loss: 0.4743\n",
      "Epoch 173, Train Loss: 0.4705, Valid Loss: 0.4725\n",
      "Epoch 174, Train Loss: 0.4686, Valid Loss: 0.4762\n",
      "Epoch 175, Train Loss: 0.4705, Valid Loss: 0.4788\n",
      "Epoch 176, Train Loss: 0.4691, Valid Loss: 0.4754\n",
      "Epoch 177, Train Loss: 0.4671, Valid Loss: 0.4803\n",
      "Epoch 178, Train Loss: 0.4686, Valid Loss: 0.4811\n",
      "Epoch 179, Train Loss: 0.4740, Valid Loss: 0.4776\n",
      "Epoch 180, Train Loss: 0.4694, Valid Loss: 0.4734\n",
      "Epoch 181, Train Loss: 0.4650, Valid Loss: 0.4727\n",
      "Epoch 182, Train Loss: 0.4699, Valid Loss: 0.4732\n",
      "Epoch 183, Train Loss: 0.4645, Valid Loss: 0.4745\n",
      "Epoch 184, Train Loss: 0.4652, Valid Loss: 0.4747\n",
      "Epoch 185, Train Loss: 0.4627, Valid Loss: 0.4754\n",
      "Epoch 186, Train Loss: 0.4647, Valid Loss: 0.4754\n",
      "Epoch 187, Train Loss: 0.4645, Valid Loss: 0.4755\n",
      "Epoch 188, Train Loss: 0.4637, Valid Loss: 0.4762\n",
      "Epoch 189, Train Loss: 0.4675, Valid Loss: 0.4749\n",
      "Epoch 190, Train Loss: 0.4639, Valid Loss: 0.4762\n",
      "Epoch 191, Train Loss: 0.4653, Valid Loss: 0.4754\n",
      "Epoch 192, Train Loss: 0.4652, Valid Loss: 0.4748\n",
      "Epoch 193, Train Loss: 0.4635, Valid Loss: 0.4733\n",
      "Epoch 194, Train Loss: 0.4657, Valid Loss: 0.4729\n",
      "Epoch 195, Train Loss: 0.4638, Valid Loss: 0.4736\n",
      "Epoch 196, Train Loss: 0.4618, Valid Loss: 0.4763\n",
      "Epoch 197, Train Loss: 0.4632, Valid Loss: 0.4756\n",
      "Epoch 198, Train Loss: 0.4628, Valid Loss: 0.4751\n",
      "Epoch 199, Train Loss: 0.4641, Valid Loss: 0.4794\n",
      "Epoch 200, Train Loss: 0.4626, Valid Loss: 0.4758\n",
      "‚úÖ Training complete.\n",
      "‚öôÔ∏è Generating final predictions...\n",
      "\n",
      "=== PyTorch Geometric GNN Evaluation ===\n",
      "\n",
      "üìä Multilabel Evaluation Metrics:\n",
      "Nom :               : Train (GNN PyG)\n",
      "Subset accuracy     : 0.0053\n",
      "Hamming loss        : 0.2762\n",
      "Micro F1            : 0.7704\n",
      "Macro F1            : 0.5775\n",
      "Weighted F1         : 0.7425\n",
      "Micro ROC-AUC       : 0.7757\n",
      "Macro ROC-AUC       : 0.5160\n",
      "\n",
      "üìä Multilabel Evaluation Metrics:\n",
      "Nom :               : Validation (GNN PyG)\n",
      "Subset accuracy     : 0.0490\n",
      "Hamming loss        : 0.2155\n",
      "Micro F1            : 0.8351\n",
      "Macro F1            : 0.6315\n",
      "Weighted F1         : 0.8070\n",
      "Micro ROC-AUC       : 0.8367\n",
      "Macro ROC-AUC       : 0.5680\n",
      "\n",
      "üìä Multilabel Evaluation Metrics:\n",
      "Nom :               : Test (GNN PyG)\n",
      "Subset accuracy     : 0.0350\n",
      "Hamming loss        : 0.2199\n",
      "Micro F1            : 0.8213\n",
      "Macro F1            : 0.6136\n",
      "Weighted F1         : 0.7989\n",
      "Micro ROC-AUC       : 0.8391\n",
      "Macro ROC-AUC       : 0.6014\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool # Example layers\n",
    "\n",
    "# --- 6. Define the PyTorch Geometric GNN ---\n",
    "class SiderGNN_PyG(nn.Module):\n",
    "    def __init__(self, n_tasks, n_node_features, hidden_channels=128, dense_dim=256, dropout=0.3, num_gc_layers=None): # Increased dimensions\n",
    "        super(SiderGNN_PyG, self).__init__()\n",
    "        self.dropout_p = dropout\n",
    "        self.n_tasks = n_tasks\n",
    "\n",
    "        # GCN Layers (Wider)\n",
    "        self.conv1 = GCNConv(n_node_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        # self.conv3 = GCNConv(hidden_channels, hidden_channels) # Option: Add a third layer if needed\n",
    "\n",
    "        # Dense Layers after pooling (Wider)\n",
    "        self.dense1 = nn.Linear(hidden_channels, dense_dim) # Input from pooling\n",
    "        self.dropout = nn.Dropout(p=self.dropout_p) # Increased dropout slightly\n",
    "        self.output_layer = nn.Linear(dense_dim, n_tasks)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # Graph Convolutions\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # x = self.conv3(x, edge_index) # Option: Add third layer\n",
    "        # x = F.relu(x)\n",
    "\n",
    "        # Global Pooling\n",
    "        x_pooled = global_mean_pool(x, batch)\n",
    "\n",
    "        # Dense layers\n",
    "        x = self.dense1(x_pooled)\n",
    "        x = F.relu(x)\n",
    "        if self.dropout_p > 0:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Output logits\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n",
    "\n",
    "# --- 7. Instantiate Model, Loss, Optimizer ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "n_node_features = train_pyg_dataset[0].num_node_features # Get feature dim from first sample\n",
    "n_tasks = len(sider_tasks)\n",
    "\n",
    "model_pyg = SiderGNN_PyG(n_tasks=n_tasks, n_node_features=n_node_features).to(device)\n",
    "optimizer = optim.Adam(model_pyg.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss() # Multi-label binary classification loss\n",
    "\n",
    "print(\"‚úÖ PyTorch Geometric GNN model instantiated.\")\n",
    "\n",
    "# --- 8. Training Loop ---\n",
    "n_epochs = 200 # Adjust as needed\n",
    "\n",
    "print(f\"üöÄ Training PyTorch Geometric GNN for {n_epochs} epochs...\")\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, data.y) # PyG Data objects store 'y' directly\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def eval_model(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, data.y)\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            all_logits.append(logits.cpu().numpy())\n",
    "            all_labels.append(data.y.cpu().numpy()) # Store true labels too\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    logits_array = np.concatenate(all_logits, axis=0)\n",
    "    labels_array = np.concatenate(all_labels, axis=0) # Shape might be [N, 1, n_tasks], need reshape if so\n",
    "\n",
    "    # Reshape labels if necessary (if they are [N, 1, n_tasks])\n",
    "    if labels_array.ndim == 3 and labels_array.shape[1] == 1:\n",
    "        labels_array = labels_array.squeeze(1)\n",
    "\n",
    "    return avg_loss, logits_array, labels_array\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = train_epoch(model_pyg, train_loader, criterion, optimizer, device)\n",
    "    valid_loss, valid_logits, _ = eval_model(model_pyg, valid_loader, criterion, device) # Ignoring labels from eval here\n",
    "    print(f\"Epoch {epoch:02d}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Training complete.\")\n",
    "\n",
    "# --- 9. Final Evaluation ---\n",
    "print(\"‚öôÔ∏è Generating final predictions...\")\n",
    "_, train_logits_final, _ = eval_model(model_pyg, train_loader, criterion, device)\n",
    "_, valid_logits_final, _ = eval_model(model_pyg, valid_loader, criterion, device)\n",
    "_, test_logits_final, test_labels_final = eval_model(model_pyg, test_loader, criterion, device) # Get test labels here\n",
    "\n",
    "# Use y_true arrays extracted earlier for consistency with evaluation function\n",
    "# assert np.array_equal(test_labels_final, y_test_true) # Optional check\n",
    "\n",
    "# Convert final logits to probabilities and predictions\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "y_train_prob_pyg = sigmoid(train_logits_final)\n",
    "y_valid_prob_pyg = sigmoid(valid_logits_final)\n",
    "y_test_prob_pyg = sigmoid(test_logits_final)\n",
    "\n",
    "y_train_pred_pyg = (y_train_prob_pyg >= 0.5).astype(int)\n",
    "y_valid_pred_pyg = (y_valid_prob_pyg >= 0.5).astype(int)\n",
    "y_test_pred_pyg = (y_test_prob_pyg >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== PyTorch Geometric GNN Evaluation ===\")\n",
    "metrics_train_pyg = evaluate_multilabel_model(y_train_true, y_train_pred_pyg, y_train_prob_pyg, \"Train (GNN PyG)\")\n",
    "metrics_valid_pyg = evaluate_multilabel_model(y_valid_true, y_valid_pred_pyg, y_valid_prob_pyg, \"Validation (GNN PyG)\")\n",
    "metrics_test_pyg = evaluate_multilabel_model(y_test_true, y_test_pred_pyg, y_test_prob_pyg, \"Test (GNN PyG)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6109972-8077-412c-be16-e3479ea74220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import gc\n",
    "\n",
    "# --- Define the Objective Function for Optuna ---\n",
    "def objective(trial):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    n_node_features = train_pyg_dataset[0].num_node_features\n",
    "    n_tasks = len(sider_tasks)\n",
    "\n",
    "    # --- Hyperparameters to Tune ---\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', [64, 128, 256])\n",
    "    dense_dim = trial.suggest_categorical('dense_dim', [128, 256, 512])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "    num_gc_layers = trial.suggest_int('num_gc_layers', 1, 3) # Test 1, 2, or 3 GCN layers\n",
    "    n_epochs_objective = 100 # Fixed number of epochs for optimization trial (adjust if needed)\n",
    "\n",
    "    # --- Instantiate Model, Loss, Optimizer ---\n",
    "    model = SiderGNN_PyG(\n",
    "        n_tasks=n_tasks,\n",
    "        n_node_features=n_node_features,\n",
    "        hidden_channels=hidden_channels,\n",
    "        dense_dim=dense_dim,\n",
    "        dropout=dropout,\n",
    "        num_gc_layers=num_gc_layers\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # --- Training Loop (with potential simple early stopping based on valid loss) ---\n",
    "    best_valid_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 25 # Stop after 25 epochs with no improvement in validation loss\n",
    "\n",
    "    for epoch in range(1, n_epochs_objective + 1):\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        valid_loss, valid_logits, _ = eval_model(model, valid_loader, criterion, device)\n",
    "\n",
    "        # Simple Early Stopping Check\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # Report intermediate results to Optuna (optional, helps pruning)\n",
    "        trial.report(valid_loss, epoch)\n",
    "\n",
    "        # Handle pruning based on intermediate value\n",
    "        if trial.should_prune():\n",
    "            print(f\"Trial {trial.number} pruned at epoch {epoch}.\")\n",
    "            # Clean up memory before raising TrialPruned\n",
    "            del model, optimizer, criterion\n",
    "            gc.collect()\n",
    "            if device == 'cuda': torch.cuda.empty_cache()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch} for trial {trial.number}.\")\n",
    "            break\n",
    "\n",
    "        # Print progress less frequently\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Trial {trial.number}, Epoch {epoch}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # --- Evaluate Final Performance on Validation Set for this trial ---\n",
    "    # We want to maximize Macro F1 (or Macro AUC)\n",
    "    _, final_valid_logits, final_valid_labels = eval_model(model, valid_loader, criterion, device)\n",
    "    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "    final_valid_probs = sigmoid(final_valid_logits)\n",
    "    final_valid_preds = (final_valid_probs >= 0.5).astype(int)\n",
    "\n",
    "    # Reshape labels if necessary (e.g., if [N, 1, n_tasks])\n",
    "    if final_valid_labels.ndim == 3 and final_valid_labels.shape[1] == 1:\n",
    "        final_valid_labels = final_valid_labels.squeeze(1)\n",
    "\n",
    "    # Calculate the metric to optimize (e.g., Macro F1)\n",
    "    macro_f1_valid = f1_score(final_valid_labels, final_valid_preds, average=\"macro\", zero_division=0)\n",
    "    # macro_auc_valid = roc_auc_score(final_valid_labels, final_valid_probs, average=\"macro\") # Alternative\n",
    "\n",
    "    # Clean up memory\n",
    "    del model, optimizer, criterion, final_valid_logits, final_valid_probs, final_valid_preds\n",
    "    gc.collect()\n",
    "    if device == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "    return macro_f1_valid # Optuna tries to maximize this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69eb3c04-c675-4975-9fea-bd35c6d9a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:31:55,754] Using an existing study with name 'sider-gnn-pyg-optimization' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Optuna Bayesian Optimization for 50 trials...\n",
      "Trial 3, Epoch 10, Train Loss: 0.5202, Valid Loss: 0.4764\n",
      "Trial 3, Epoch 20, Train Loss: 0.5152, Valid Loss: 0.4790\n",
      "Trial 3, Epoch 30, Train Loss: 0.5142, Valid Loss: 0.4777\n",
      "Trial 3, Epoch 40, Train Loss: 0.5095, Valid Loss: 0.4773\n",
      "Trial 3, Epoch 50, Train Loss: 0.5077, Valid Loss: 0.4764\n",
      "Trial 3, Epoch 60, Train Loss: 0.5044, Valid Loss: 0.4731\n",
      "Trial 3, Epoch 70, Train Loss: 0.5039, Valid Loss: 0.4746\n",
      "Early stopping triggered at epoch 80 for trial 3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:32:22,474] Trial 3 finished with value: 0.6331369495601278 and parameters: {'lr': 0.0007577676387152359, 'hidden_channels': 64, 'dense_dim': 256, 'dropout': 0.26115381334077103, 'num_gc_layers': 2}. Best is trial 3 with value: 0.6331369495601278.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Epoch 10, Train Loss: 0.5330, Valid Loss: 0.4827\n",
      "Trial 4, Epoch 20, Train Loss: 0.5278, Valid Loss: 0.4795\n",
      "Trial 4, Epoch 30, Train Loss: 0.5217, Valid Loss: 0.4779\n",
      "Trial 4, Epoch 40, Train Loss: 0.5214, Valid Loss: 0.4780\n",
      "Trial 4, Epoch 50, Train Loss: 0.5195, Valid Loss: 0.4772\n",
      "Trial 4, Epoch 60, Train Loss: 0.5171, Valid Loss: 0.4786\n",
      "Trial 4, Epoch 70, Train Loss: 0.5164, Valid Loss: 0.4782\n",
      "Trial 4, Epoch 80, Train Loss: 0.5147, Valid Loss: 0.4761\n",
      "Trial 4, Epoch 90, Train Loss: 0.5145, Valid Loss: 0.4764\n",
      "Trial 4, Epoch 100, Train Loss: 0.5137, Valid Loss: 0.4755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:34:09,956] Trial 4 finished with value: 0.636597281845318 and parameters: {'lr': 0.00018166245646355152, 'hidden_channels': 256, 'dense_dim': 128, 'dropout': 0.4096553087655359, 'num_gc_layers': 1}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Epoch 10, Train Loss: 0.5217, Valid Loss: 0.4781\n",
      "Trial 5, Epoch 20, Train Loss: 0.5149, Valid Loss: 0.4774\n",
      "Trial 5, Epoch 30, Train Loss: 0.5123, Valid Loss: 0.4747\n",
      "Trial 5, Epoch 40, Train Loss: 0.5101, Valid Loss: 0.4766\n",
      "Trial 5, Epoch 50, Train Loss: 0.5084, Valid Loss: 0.4755\n",
      "Trial 5, Epoch 60, Train Loss: 0.5064, Valid Loss: 0.4740\n",
      "Trial 5, Epoch 70, Train Loss: 0.5028, Valid Loss: 0.4732\n",
      "Trial 5, Epoch 80, Train Loss: 0.4997, Valid Loss: 0.4782\n",
      "Trial 5, Epoch 90, Train Loss: 0.4999, Valid Loss: 0.4719\n",
      "Trial 5, Epoch 100, Train Loss: 0.4981, Valid Loss: 0.4779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:34:51,908] Trial 5 finished with value: 0.6280169343766983 and parameters: {'lr': 0.0004316637250276161, 'hidden_channels': 64, 'dense_dim': 512, 'dropout': 0.383652696906138, 'num_gc_layers': 3}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Epoch 10, Train Loss: 0.5206, Valid Loss: 0.4762\n",
      "Trial 6, Epoch 20, Train Loss: 0.5152, Valid Loss: 0.4789\n",
      "Trial 6, Epoch 30, Train Loss: 0.5095, Valid Loss: 0.4754\n",
      "Trial 6, Epoch 40, Train Loss: 0.5041, Valid Loss: 0.4738\n",
      "Trial 6, Epoch 50, Train Loss: 0.4984, Valid Loss: 0.4742\n",
      "Trial 6, Epoch 60, Train Loss: 0.4979, Valid Loss: 0.4717\n",
      "Trial 6, Epoch 70, Train Loss: 0.4924, Valid Loss: 0.4735\n",
      "Trial 6, Epoch 80, Train Loss: 0.4892, Valid Loss: 0.4741\n",
      "Early stopping triggered at epoch 86 for trial 6.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:35:44,061] Trial 6 finished with value: 0.6355717502183398 and parameters: {'lr': 0.0018404959877121624, 'hidden_channels': 128, 'dense_dim': 256, 'dropout': 0.45833289009811184, 'num_gc_layers': 2}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Epoch 10, Train Loss: 0.5213, Valid Loss: 0.4799\n",
      "Trial 7, Epoch 20, Train Loss: 0.5174, Valid Loss: 0.4752\n",
      "Trial 7, Epoch 30, Train Loss: 0.5145, Valid Loss: 0.4787\n",
      "Trial 7, Epoch 40, Train Loss: 0.5143, Valid Loss: 0.4774\n",
      "Early stopping triggered at epoch 50 for trial 7.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:36:15,325] Trial 7 finished with value: 0.6296678125568748 and parameters: {'lr': 0.0002291594758135411, 'hidden_channels': 128, 'dense_dim': 512, 'dropout': 0.2965010570142653, 'num_gc_layers': 2}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:36:16,496] Trial 8 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:36:17,630] Trial 9 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:36:18,536] Trial 10 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11, Epoch 10, Train Loss: 0.5319, Valid Loss: 0.4867\n",
      "Trial 11, Epoch 20, Train Loss: 0.5260, Valid Loss: 0.4816\n",
      "Trial 11, Epoch 30, Train Loss: 0.5222, Valid Loss: 0.4797\n",
      "Trial 11, Epoch 40, Train Loss: 0.5202, Valid Loss: 0.4784\n",
      "Trial 11, Epoch 50, Train Loss: 0.5185, Valid Loss: 0.4776\n",
      "Trial 11, Epoch 60, Train Loss: 0.5167, Valid Loss: 0.4778\n",
      "Trial 11, Epoch 70, Train Loss: 0.5155, Valid Loss: 0.4771\n",
      "Trial 11, Epoch 80, Train Loss: 0.5143, Valid Loss: 0.4766\n",
      "Trial 11, Epoch 90, Train Loss: 0.5130, Valid Loss: 0.4773\n",
      "Trial 11, Epoch 100, Train Loss: 0.5133, Valid Loss: 0.4762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:36:57,238] Trial 11 finished with value: 0.6345147998205051 and parameters: {'lr': 0.00013194461230656444, 'hidden_channels': 64, 'dense_dim': 256, 'dropout': 0.3852186691853984, 'num_gc_layers': 1}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:36:58,151] Trial 12 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:36:59,683] Trial 13 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:37:01,212] Trial 14 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15, Epoch 10, Train Loss: 0.5342, Valid Loss: 0.4851\n",
      "Trial 15, Epoch 20, Train Loss: 0.5276, Valid Loss: 0.4806\n",
      "Trial 15, Epoch 30, Train Loss: 0.5236, Valid Loss: 0.4791\n",
      "Trial 15, Epoch 40, Train Loss: 0.5230, Valid Loss: 0.4788\n",
      "Trial 15, Epoch 50, Train Loss: 0.5207, Valid Loss: 0.4778\n",
      "Trial 15, Epoch 60, Train Loss: 0.5187, Valid Loss: 0.4771\n",
      "Trial 15, Epoch 70, Train Loss: 0.5178, Valid Loss: 0.4775\n",
      "Trial 15, Epoch 80, Train Loss: 0.5171, Valid Loss: 0.4766\n",
      "Trial 15, Epoch 90, Train Loss: 0.5152, Valid Loss: 0.4765\n",
      "Early stopping triggered at epoch 91 for trial 15.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:38:41,880] Trial 15 finished with value: 0.6346449911229528 and parameters: {'lr': 0.00010649438499695516, 'hidden_channels': 256, 'dense_dim': 256, 'dropout': 0.4407403167482229, 'num_gc_layers': 2}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:38:42,903] Trial 16 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:38:44,450] Trial 17 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:38:45,553] Trial 18 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:38:47,183] Trial 19 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20, Epoch 10, Train Loss: 0.5306, Valid Loss: 0.4820\n",
      "Trial 20, Epoch 20, Train Loss: 0.5241, Valid Loss: 0.4797\n",
      "Trial 20, Epoch 30, Train Loss: 0.5208, Valid Loss: 0.4754\n",
      "Trial 20, Epoch 40, Train Loss: 0.5180, Valid Loss: 0.4764\n",
      "Trial 20, Epoch 50, Train Loss: 0.5161, Valid Loss: 0.4762\n",
      "Early stopping triggered at epoch 55 for trial 20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:39:24,803] Trial 20 finished with value: 0.6330486707138062 and parameters: {'lr': 0.00020642043006316308, 'hidden_channels': 128, 'dense_dim': 256, 'dropout': 0.49783456492607947, 'num_gc_layers': 2}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 21 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:39:27,199] Trial 21 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 22 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:39:28,668] Trial 22 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 23 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:39:30,588] Trial 23 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 24, Epoch 10, Train Loss: 0.5315, Valid Loss: 0.4831\n",
      "Trial 24, Epoch 20, Train Loss: 0.5259, Valid Loss: 0.4804\n",
      "Trial 24, Epoch 30, Train Loss: 0.5228, Valid Loss: 0.4801\n",
      "Trial 24, Epoch 40, Train Loss: 0.5195, Valid Loss: 0.4774\n",
      "Trial 24, Epoch 50, Train Loss: 0.5177, Valid Loss: 0.4772\n",
      "Trial 24, Epoch 60, Train Loss: 0.5175, Valid Loss: 0.4803\n",
      "Trial 24, Epoch 70, Train Loss: 0.5167, Valid Loss: 0.4801\n",
      "Trial 24, Epoch 80, Train Loss: 0.5140, Valid Loss: 0.4792\n",
      "Early stopping triggered at epoch 84 for trial 24.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:41:04,715] Trial 24 finished with value: 0.633970144183028 and parameters: {'lr': 0.0001237071389803396, 'hidden_channels': 256, 'dense_dim': 256, 'dropout': 0.44217072142545116, 'num_gc_layers': 2}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:41:06,219] Trial 25 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26, Epoch 10, Train Loss: 0.5350, Valid Loss: 0.4856\n",
      "Trial 26, Epoch 20, Train Loss: 0.5287, Valid Loss: 0.4818\n",
      "Trial 26, Epoch 30, Train Loss: 0.5237, Valid Loss: 0.4789\n",
      "Trial 26, Epoch 40, Train Loss: 0.5232, Valid Loss: 0.4792\n",
      "Trial 26, Epoch 50, Train Loss: 0.5199, Valid Loss: 0.4775\n",
      "Trial 26, Epoch 60, Train Loss: 0.5183, Valid Loss: 0.4764\n",
      "Trial 26, Epoch 70, Train Loss: 0.5173, Valid Loss: 0.4790\n",
      "Trial 26, Epoch 80, Train Loss: 0.5154, Valid Loss: 0.4776\n",
      "Early stopping triggered at epoch 88 for trial 26.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:42:43,439] Trial 26 finished with value: 0.6321091914354153 and parameters: {'lr': 0.00010182170609672667, 'hidden_channels': 256, 'dense_dim': 256, 'dropout': 0.40524717527736404, 'num_gc_layers': 3}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 27 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:42:44,918] Trial 27 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28, Epoch 10, Train Loss: 0.5287, Valid Loss: 0.4821\n",
      "Trial 28, Epoch 20, Train Loss: 0.5232, Valid Loss: 0.4804\n",
      "Trial 28, Epoch 30, Train Loss: 0.5220, Valid Loss: 0.4786\n",
      "Trial 28, Epoch 40, Train Loss: 0.5186, Valid Loss: 0.4780\n",
      "Trial 28, Epoch 50, Train Loss: 0.5183, Valid Loss: 0.4780\n",
      "Trial 28, Epoch 60, Train Loss: 0.5158, Valid Loss: 0.4770\n",
      "Early stopping triggered at epoch 68 for trial 28.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:43:24,863] Trial 28 finished with value: 0.6309284484849627 and parameters: {'lr': 0.00015382625703583337, 'hidden_channels': 128, 'dense_dim': 256, 'dropout': 0.3622031044172216, 'num_gc_layers': 2}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 29, Epoch 10, Train Loss: 0.5373, Valid Loss: 0.4868\n",
      "Trial 29, Epoch 20, Train Loss: 0.5282, Valid Loss: 0.4806\n",
      "Trial 29, Epoch 30, Train Loss: 0.5251, Valid Loss: 0.4797\n",
      "Trial 29, Epoch 40, Train Loss: 0.5221, Valid Loss: 0.4797\n",
      "Trial 29, Epoch 50, Train Loss: 0.5214, Valid Loss: 0.4777\n",
      "Trial 29, Epoch 60, Train Loss: 0.5200, Valid Loss: 0.4782\n",
      "Trial 29, Epoch 70, Train Loss: 0.5174, Valid Loss: 0.4783\n",
      "Trial 29, Epoch 80, Train Loss: 0.5177, Valid Loss: 0.4782\n",
      "Trial 29, Epoch 90, Train Loss: 0.5167, Valid Loss: 0.4785\n",
      "Trial 29, Epoch 100, Train Loss: 0.5153, Valid Loss: 0.4780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:45:12,688] Trial 29 finished with value: 0.6344205596010943 and parameters: {'lr': 0.00010131123897784368, 'hidden_channels': 256, 'dense_dim': 256, 'dropout': 0.47151170065780657, 'num_gc_layers': 3}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:45:13,806] Trial 30 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 31 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:45:15,398] Trial 31 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 32 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:45:16,399] Trial 32 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 33 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:45:17,874] Trial 33 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 34, Epoch 10, Train Loss: 0.5341, Valid Loss: 0.4860\n",
      "Trial 34, Epoch 20, Train Loss: 0.5266, Valid Loss: 0.4808\n",
      "Trial 34, Epoch 30, Train Loss: 0.5231, Valid Loss: 0.4794\n",
      "Trial 34, Epoch 40, Train Loss: 0.5199, Valid Loss: 0.4785\n",
      "Trial 34, Epoch 50, Train Loss: 0.5194, Valid Loss: 0.4780\n",
      "Trial 34, Epoch 60, Train Loss: 0.5174, Valid Loss: 0.4766\n",
      "Trial 34, Epoch 70, Train Loss: 0.5171, Valid Loss: 0.4764\n",
      "Trial 34, Epoch 80, Train Loss: 0.5154, Valid Loss: 0.4765\n",
      "Trial 34, Epoch 90, Train Loss: 0.5154, Valid Loss: 0.4774\n",
      "Trial 34, Epoch 100, Train Loss: 0.5134, Valid Loss: 0.4767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:45:56,641] Trial 34 finished with value: 0.6338332615749807 and parameters: {'lr': 0.00013224539007653495, 'hidden_channels': 64, 'dense_dim': 256, 'dropout': 0.4115247796822336, 'num_gc_layers': 1}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 35, Epoch 10, Train Loss: 0.5345, Valid Loss: 0.4869\n",
      "Trial 35, Epoch 20, Train Loss: 0.5256, Valid Loss: 0.4809\n",
      "Trial 35, Epoch 30, Train Loss: 0.5246, Valid Loss: 0.4805\n",
      "Trial 35, Epoch 40, Train Loss: 0.5226, Valid Loss: 0.4805\n",
      "Trial 35, Epoch 50, Train Loss: 0.5206, Valid Loss: 0.4789\n",
      "Trial 35, Epoch 60, Train Loss: 0.5184, Valid Loss: 0.4787\n",
      "Trial 35, Epoch 70, Train Loss: 0.5177, Valid Loss: 0.4773\n",
      "Trial 35, Epoch 80, Train Loss: 0.5168, Valid Loss: 0.4780\n",
      "Trial 35, Epoch 90, Train Loss: 0.5166, Valid Loss: 0.4778\n",
      "Trial 35, Epoch 100, Train Loss: 0.5148, Valid Loss: 0.4778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:46:35,758] Trial 35 finished with value: 0.6329771749657422 and parameters: {'lr': 0.0001431085505114432, 'hidden_channels': 64, 'dense_dim': 256, 'dropout': 0.3880748219696136, 'num_gc_layers': 1}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 36 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:46:36,646] Trial 36 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 37 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:46:37,526] Trial 37 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 38, Epoch 10, Train Loss: 0.5471, Valid Loss: 0.4955\n",
      "Trial 38, Epoch 20, Train Loss: 0.5299, Valid Loss: 0.4845\n",
      "Trial 38, Epoch 30, Train Loss: 0.5266, Valid Loss: 0.4814\n",
      "Trial 38, Epoch 40, Train Loss: 0.5257, Valid Loss: 0.4797\n",
      "Trial 38, Epoch 50, Train Loss: 0.5232, Valid Loss: 0.4793\n",
      "Trial 38, Epoch 60, Train Loss: 0.5220, Valid Loss: 0.4800\n",
      "Trial 38, Epoch 70, Train Loss: 0.5205, Valid Loss: 0.4784\n",
      "Trial 38, Epoch 80, Train Loss: 0.5198, Valid Loss: 0.4792\n",
      "Trial 38, Epoch 90, Train Loss: 0.5205, Valid Loss: 0.4782\n",
      "Trial 38, Epoch 100, Train Loss: 0.5172, Valid Loss: 0.4776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:47:17,356] Trial 38 finished with value: 0.6332512478174548 and parameters: {'lr': 0.00010238489617897034, 'hidden_channels': 64, 'dense_dim': 256, 'dropout': 0.46217805206942736, 'num_gc_layers': 1}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 39 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:47:18,278] Trial 39 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 40 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:47:19,320] Trial 40 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 41 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:47:20,207] Trial 41 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 42, Epoch 10, Train Loss: 0.5305, Valid Loss: 0.4814\n",
      "Trial 42, Epoch 20, Train Loss: 0.5225, Valid Loss: 0.4785\n",
      "Trial 42, Epoch 30, Train Loss: 0.5205, Valid Loss: 0.4791\n",
      "Trial 42, Epoch 40, Train Loss: 0.5198, Valid Loss: 0.4783\n",
      "Trial 42, Epoch 50, Train Loss: 0.5160, Valid Loss: 0.4773\n",
      "Trial 42, Epoch 60, Train Loss: 0.5148, Valid Loss: 0.4779\n",
      "Early stopping triggered at epoch 70 for trial 42.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:48:36,832] Trial 42 finished with value: 0.6347263974891398 and parameters: {'lr': 0.00016308986897753982, 'hidden_channels': 256, 'dense_dim': 256, 'dropout': 0.3952458632835102, 'num_gc_layers': 1}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 43 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:48:38,446] Trial 43 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 44 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:48:39,963] Trial 44 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 45, Epoch 10, Train Loss: 0.5320, Valid Loss: 0.4833\n",
      "Trial 45, Epoch 20, Train Loss: 0.5264, Valid Loss: 0.4795\n",
      "Trial 45, Epoch 30, Train Loss: 0.5222, Valid Loss: 0.4782\n",
      "Trial 45, Epoch 40, Train Loss: 0.5215, Valid Loss: 0.4784\n",
      "Trial 45, Epoch 50, Train Loss: 0.5178, Valid Loss: 0.4787\n",
      "Trial 45, Epoch 60, Train Loss: 0.5181, Valid Loss: 0.4769\n",
      "Trial 45, Epoch 70, Train Loss: 0.5169, Valid Loss: 0.4782\n",
      "Trial 45, Epoch 80, Train Loss: 0.5149, Valid Loss: 0.4782\n",
      "Trial 45, Epoch 90, Train Loss: 0.5147, Valid Loss: 0.4773\n",
      "Trial 45, Epoch 100, Train Loss: 0.5118, Valid Loss: 0.4775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:50:30,146] Trial 45 finished with value: 0.6336959059009467 and parameters: {'lr': 0.00012143151316247146, 'hidden_channels': 256, 'dense_dim': 256, 'dropout': 0.4271227977257237, 'num_gc_layers': 1}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 46 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:50:31,670] Trial 46 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 47 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:50:33,174] Trial 47 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 48 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:50:34,221] Trial 48 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 49 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:50:35,136] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:50:36,783] Trial 50 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 51 pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:50:37,903] Trial 51 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 52, Epoch 10, Train Loss: 0.5402, Valid Loss: 0.4876\n",
      "Trial 52, Epoch 20, Train Loss: 0.5314, Valid Loss: 0.4837\n",
      "Trial 52, Epoch 30, Train Loss: 0.5279, Valid Loss: 0.4806\n",
      "Trial 52, Epoch 40, Train Loss: 0.5243, Valid Loss: 0.4791\n",
      "Trial 52, Epoch 50, Train Loss: 0.5218, Valid Loss: 0.4780\n",
      "Trial 52, Epoch 60, Train Loss: 0.5224, Valid Loss: 0.4802\n",
      "Trial 52, Epoch 70, Train Loss: 0.5192, Valid Loss: 0.4781\n",
      "Trial 52, Epoch 80, Train Loss: 0.5183, Valid Loss: 0.4793\n",
      "Early stopping triggered at epoch 84 for trial 52.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 16:52:08,002] Trial 52 finished with value: 0.6344442710457535 and parameters: {'lr': 0.0001264675905844007, 'hidden_channels': 256, 'dense_dim': 128, 'dropout': 0.42525667965217473, 'num_gc_layers': 1}. Best is trial 4 with value: 0.636597281845318.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Optimization Finished!\n",
      "Number of finished trials: 53\n",
      "Best trial:\n",
      "  Value (Max Macro F1 on Validation): 0.6366\n",
      "  Params: \n",
      "    lr: 0.00018166245646355152\n",
      "    hidden_channels: 256\n",
      "    dense_dim: 128\n",
      "    dropout: 0.4096553087655359\n",
      "    num_gc_layers: 1\n"
     ]
    }
   ],
   "source": [
    "# --- Create and Run the Optuna Study ---\n",
    "study_name = \"sider-gnn-pyg-optimization\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\" # Saves results to a file\n",
    "\n",
    "# We aim to maximize Macro F1\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage_name, # Use SQLite storage to resume if interrupted\n",
    "    load_if_exists=True,   # Load previous results if the db file exists\n",
    "    direction=\"maximize\"\n",
    ")\n",
    "\n",
    "n_trials = 50 # Number of hyperparameter sets to test (adjust based on time/resources)\n",
    "\n",
    "print(f\"üöÄ Starting Optuna Bayesian Optimization for {n_trials} trials...\")\n",
    "study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "# --- Output Best Results ---\n",
    "print(\"\\n‚úÖ Optimization Finished!\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value (Max Macro F1 on Validation): {trial.value:.4f}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66de7aa0-d39b-4a6d-be52-8f5ba387caaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Using Best Hyperparameters: {'lr': 0.00018166245646355152, 'hidden_channels': 256, 'dense_dim': 128, 'dropout': 0.4096553087655359, 'num_gc_layers': 1}\n",
      "‚úÖ Final GNN model instantiated on cpu.\n",
      "üöÄ Training Final GNN for 100 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:28<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final Training complete.\n",
      "‚öôÔ∏è Evaluating Final Model on Test Set...\n",
      "\n",
      "üìä Multilabel Evaluation Metrics:\n",
      "Nom :               : Test (Best GNN PyG)\n",
      "Subset accuracy     : 0.0280\n",
      "Hamming loss        : 0.2328\n",
      "Micro F1            : 0.8175\n",
      "Macro F1            : 0.6109\n",
      "Weighted F1         : 0.7964\n",
      "Micro ROC-AUC       : 0.8301\n",
      "Macro ROC-AUC       : 0.5512\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Get Best Hyperparameters ---\n",
    "if 'study' not in locals(): raise NameError(\"Optuna 'study' object not found.\")\n",
    "best_params = study.best_params\n",
    "print(\"üìã Using Best Hyperparameters:\", best_params)\n",
    "\n",
    "# --- 2. Instantiate Final Model ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_node_features = train_pyg_dataset[0].num_node_features\n",
    "n_tasks = len(sider_tasks)\n",
    "\n",
    "final_model = SiderGNN_PyG( # Make sure SiderGNN_PyG_Opt class is defined\n",
    "    n_tasks=n_tasks, n_node_features=n_node_features,\n",
    "    hidden_channels=best_params['hidden_channels'], dense_dim=best_params['dense_dim'],\n",
    "    dropout=best_params['dropout'], num_gc_layers=best_params['num_gc_layers']\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "print(f\"‚úÖ Final GNN model instantiated on {device}.\")\n",
    "\n",
    "# --- 3. Final Training Loop ---\n",
    "n_epochs_final = 100 # Adjust as needed (e.g., based on Optuna trial duration)\n",
    "print(f\"üöÄ Training Final GNN for {n_epochs_final} epochs...\")\n",
    "for epoch in tqdm(range(1, n_epochs_final + 1), desc=\"Final Training\"):\n",
    "    train_loss = train_epoch(final_model, train_loader, criterion, optimizer, device) # Assumes train_epoch is defined\n",
    "    # Optional: print occasional validation loss\n",
    "    # if epoch % 20 == 0:\n",
    "    #     valid_loss, _, _ = eval_model(final_model, valid_loader, criterion, device) # Assumes eval_model is defined\n",
    "    #     print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "print(\"‚úÖ Final Training complete.\")\n",
    "\n",
    "# --- 4. Final Evaluation on Test Set ---\n",
    "print(\"‚öôÔ∏è Evaluating Final Model on Test Set...\")\n",
    "# Assumes eval_model returns (avg_loss, logits_array, labels_array)\n",
    "_, test_logits_final, _ = eval_model(final_model, test_loader, criterion, device)\n",
    "\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "y_test_prob_final = sigmoid(test_logits_final)\n",
    "y_test_pred_final = (y_test_prob_final >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "metrics_test_final_gnn = evaluate_multilabel_model(\n",
    "    y_test_true, y_test_pred_final, y_test_prob_final, \"Test (Best GNN PyG)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c804e409-93bb-447c-bf4a-2b337fb12e6a",
   "metadata": {},
   "source": [
    "For this specific dataset and scaffold split, a simple GCN architecture, even after hyperparameter optimization, doesn't seem to provide a clear advantage over tuned ECFP-based models like XGBoost, particularly for predicting rare side effects. The inherent difficulty of the scaffold split and the severe label imbalance remain major challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38422c55-8357-4722-9d0a-a2f44813fc62",
   "metadata": {},
   "source": [
    "let's try a more complex GNN architecture using PyTorch Geometric. We'll make two main changes:\n",
    "\n",
    "Use GATConv Layers: Graph Attention Network (GAT) layers allow nodes to weigh the importance of their neighbors' features, which can be more expressive than the simple averaging in GCNConv.\n",
    "\n",
    "Include Edge Features: We'll add basic bond features (like bond type) to the graph representation, allowing the GNN to learn from bond information as well as atom information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc1e8b1d-6be2-4335-917e-0e4d1407470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß¨ Re-creating PyTorch Geometric datasets with edge features...\n",
      "Processing 1141 SMILES for PyG graphs with edge features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Graphs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1141/1141 [00:01<00:00, 888.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 1141 graphs.\n",
      "Processing 143 SMILES for PyG graphs with edge features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Graphs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [00:00<00:00, 808.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 143 graphs.\n",
      "Processing 143 SMILES for PyG graphs with edge features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Graphs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 143/143 [00:00<00:00, 609.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 143 graphs.\n",
      "‚úÖ PyG Datasets with edge features created.\n",
      "‚úÖ PyG DataLoaders with edge features created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def smiles_to_pyg_graph_with_edges(smiles, y):\n",
    "    \"\"\"Converts SMILES string to PyTorch Geometric Data object with edge features.\"\"\"\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "\n",
    "    # --- Atom Features (Using simpler features for clarity, adjust if needed) ---\n",
    "    xs = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        x = []\n",
    "        x.append(atom.GetAtomicNum())\n",
    "        x.append(atom.GetDegree())\n",
    "        x.append(atom.GetFormalCharge())\n",
    "        x.append(atom.GetNumRadicalElectrons())\n",
    "        x.append(atom.GetHybridization())\n",
    "        x.append(atom.GetIsAromatic())\n",
    "        x.append(atom.IsInRing())\n",
    "        # Add more features if desired...\n",
    "        xs.append(x)\n",
    "    x = torch.tensor(xs, dtype=torch.float).view(-1, len(xs[0])) # Node features\n",
    "\n",
    "    # --- Edge Index and Edge Features ---\n",
    "    edge_indices = []\n",
    "    edge_attrs = []\n",
    "    bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "                  Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        bond_type = bond_types.index(bond.GetBondType()) # Get index of bond type\n",
    "\n",
    "        # Add edge in both directions\n",
    "        edge_indices += [[i, j], [j, i]]\n",
    "        # Add bond type feature for both directions\n",
    "        edge_attrs += [[bond_type], [bond_type]]\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float).view(-1, 1) # Edge features\n",
    "\n",
    "    # --- Labels ---\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float).unsqueeze(0) # Shape [1, n_tasks]\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y_tensor)\n",
    "    return data\n",
    "\n",
    "# --- Recreate PyTorch Geometric Datasets using the new function ---\n",
    "class SIDERPyGDatasetWithEdges(Dataset): # Renamed for clarity\n",
    "    def __init__(self, dc_dataset):\n",
    "        self.smiles = dc_dataset.ids\n",
    "        self.labels = dc_dataset.y\n",
    "        self.graphs = []\n",
    "        print(f\"Processing {len(self.smiles)} SMILES for PyG graphs with edge features...\")\n",
    "        for i in tqdm(range(len(self.smiles)), desc=\"Creating Graphs\"):\n",
    "            graph = smiles_to_pyg_graph_with_edges(self.smiles[i], self.labels[i])\n",
    "            if graph is not None:\n",
    "                self.graphs.append(graph)\n",
    "        print(f\"Successfully created {len(self.graphs)} graphs.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx]\n",
    "\n",
    "print(\"üß¨ Re-creating PyTorch Geometric datasets with edge features...\")\n",
    "# Make sure train_raw_ds, valid_raw_ds, test_raw_ds exist from previous loading\n",
    "train_pyg_dataset_edge = SIDERPyGDatasetWithEdges(train_raw_ds)\n",
    "valid_pyg_dataset_edge = SIDERPyGDatasetWithEdges(valid_raw_ds)\n",
    "test_pyg_dataset_edge = SIDERPyGDatasetWithEdges(test_raw_ds)\n",
    "print(\"‚úÖ PyG Datasets with edge features created.\")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "from torch_geometric.loader import DataLoader as PyGDataLoader\n",
    "batch_size = 64\n",
    "train_loader_edge = PyGDataLoader(train_pyg_dataset_edge, batch_size=batch_size, shuffle=True)\n",
    "valid_loader_edge = PyGDataLoader(valid_pyg_dataset_edge, batch_size=batch_size, shuffle=False)\n",
    "test_loader_edge = PyGDataLoader(test_pyg_dataset_edge, batch_size=batch_size, shuffle=False)\n",
    "print(\"‚úÖ PyG DataLoaders with edge features created.\")\n",
    "\n",
    "# --- Retrieve y_true again (if needed, ensure consistency) ---\n",
    "# Assuming y_train_true, y_valid_true, y_test_true are already correctly defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a7180db-fd11-4064-8f71-7621fa249b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # --- GAT Model Definition ---\n",
    "class SiderGAT_PyG(nn.Module):\n",
    "    def __init__(self, n_tasks, n_node_features, n_edge_features, hidden_channels=128, num_heads=4, dense_dim=256, dropout=0.3):\n",
    "        super(SiderGAT_PyG, self).__init__()\n",
    "        self.dropout_p = dropout\n",
    "        self.n_tasks = n_tasks\n",
    "\n",
    "        # GAT Layers (Using 2 layers, with multiple attention heads)\n",
    "        # Note: Output dimension is hidden_channels * num_heads for the first layer if concat=True\n",
    "        self.conv1 = GATConv(n_node_features, hidden_channels, heads=num_heads, dropout=dropout, edge_dim=n_edge_features)\n",
    "        # For the second layer, input dimension must match output of first layer\n",
    "        self.conv2 = GATConv(hidden_channels * num_heads, hidden_channels, heads=1, concat=False, dropout=dropout, edge_dim=n_edge_features) # Output is just hidden_channels\n",
    "\n",
    "        # Dense Layers after pooling\n",
    "        self.dense1 = nn.Linear(hidden_channels, dense_dim) # Input from pooling\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout_p)\n",
    "        self.output_layer = nn.Linear(dense_dim, n_tasks)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # Graph Attn Convolutions\n",
    "        x = self.conv1(x, edge_index, edge_attr=edge_attr)\n",
    "        x = F.elu(x) # ELU activation often used with GAT\n",
    "        x = F.dropout(x, p=self.dropout_p, training=self.training) # Dropout between layers\n",
    "\n",
    "        x = self.conv2(x, edge_index, edge_attr=edge_attr)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        # Global Pooling (Mean pooling is common)\n",
    "        x_pooled = global_mean_pool(x, batch) # Shape: [batch_size, hidden_channels]\n",
    "\n",
    "        # Dense layers\n",
    "        x = self.dense1(x_pooled)\n",
    "        x = F.relu(x) # ReLU here\n",
    "        x = self.dropout_layer(x)\n",
    "\n",
    "        # Output logits\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9372b8bd-674c-499a-9d7c-54928601948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "‚úÖ PyTorch Geometric GAT model instantiated.\n"
     ]
    }
   ],
   "source": [
    "# --- Instantiate GAT Model, Loss, Optimizer ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get feature dimensions from the dataset\n",
    "if not train_pyg_dataset_edge: raise ValueError(\"Train dataset is empty!\")\n",
    "n_node_features = train_pyg_dataset_edge[0].num_node_features\n",
    "n_edge_features = train_pyg_dataset_edge[0].num_edge_features\n",
    "n_tasks = len(sider_tasks)\n",
    "\n",
    "# Use hyperparameters potentially better suited for GAT\n",
    "lr_gat = 0.0005 # GATs sometimes benefit from slightly lower LR initially\n",
    "hidden_gat = 128\n",
    "heads_gat = 4\n",
    "dense_gat = 256\n",
    "dropout_gat = 0.3\n",
    "\n",
    "model_gat = SiderGAT_PyG(\n",
    "    n_tasks=n_tasks,\n",
    "    n_node_features=n_node_features,\n",
    "    n_edge_features=n_edge_features,\n",
    "    hidden_channels=hidden_gat,\n",
    "    num_heads=heads_gat,\n",
    "    dense_dim=dense_gat,\n",
    "    dropout=dropout_gat\n",
    ").to(device)\n",
    "\n",
    "optimizer_gat = optim.Adam(model_gat.parameters(), lr=lr_gat)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"‚úÖ PyTorch Geometric GAT model instantiated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a26e113f-477c-4556-803a-0fef51576d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training PyTorch Geometric GAT for 100 epochs...\n",
      "Epoch 001, Train Loss: 0.5982, Valid Loss: 0.4817\n",
      "Epoch 002, Train Loss: 0.5433, Valid Loss: 0.4846\n",
      "Epoch 003, Train Loss: 0.5349, Valid Loss: 0.4846\n",
      "Epoch 004, Train Loss: 0.5313, Valid Loss: 0.4809\n",
      "Epoch 005, Train Loss: 0.5289, Valid Loss: 0.4796\n",
      "Epoch 006, Train Loss: 0.5275, Valid Loss: 0.4751\n",
      "Epoch 007, Train Loss: 0.5253, Valid Loss: 0.4846\n",
      "Epoch 008, Train Loss: 0.5266, Valid Loss: 0.4781\n",
      "Epoch 009, Train Loss: 0.5235, Valid Loss: 0.4817\n",
      "Epoch 010, Train Loss: 0.5235, Valid Loss: 0.4818\n",
      "Epoch 011, Train Loss: 0.5235, Valid Loss: 0.4806\n",
      "Epoch 012, Train Loss: 0.5228, Valid Loss: 0.4748\n",
      "Epoch 013, Train Loss: 0.5218, Valid Loss: 0.4792\n",
      "Epoch 014, Train Loss: 0.5200, Valid Loss: 0.4791\n",
      "Epoch 015, Train Loss: 0.5194, Valid Loss: 0.4774\n",
      "Epoch 016, Train Loss: 0.5188, Valid Loss: 0.4795\n",
      "Epoch 017, Train Loss: 0.5222, Valid Loss: 0.4767\n",
      "Epoch 018, Train Loss: 0.5213, Valid Loss: 0.4745\n",
      "Epoch 019, Train Loss: 0.5207, Valid Loss: 0.4769\n",
      "Epoch 020, Train Loss: 0.5187, Valid Loss: 0.4813\n",
      "Epoch 021, Train Loss: 0.5184, Valid Loss: 0.4801\n",
      "Epoch 022, Train Loss: 0.5199, Valid Loss: 0.4746\n",
      "Epoch 023, Train Loss: 0.5169, Valid Loss: 0.4776\n",
      "Epoch 024, Train Loss: 0.5190, Valid Loss: 0.4843\n",
      "Epoch 025, Train Loss: 0.5185, Valid Loss: 0.4803\n",
      "Epoch 026, Train Loss: 0.5180, Valid Loss: 0.4761\n",
      "Epoch 027, Train Loss: 0.5162, Valid Loss: 0.4745\n",
      "Epoch 028, Train Loss: 0.5169, Valid Loss: 0.4745\n",
      "Epoch 029, Train Loss: 0.5156, Valid Loss: 0.4812\n",
      "Epoch 030, Train Loss: 0.5162, Valid Loss: 0.4752\n",
      "Epoch 031, Train Loss: 0.5169, Valid Loss: 0.4736\n",
      "Epoch 032, Train Loss: 0.5186, Valid Loss: 0.4740\n",
      "Epoch 033, Train Loss: 0.5156, Valid Loss: 0.4749\n",
      "Epoch 034, Train Loss: 0.5153, Valid Loss: 0.4763\n",
      "Epoch 035, Train Loss: 0.5154, Valid Loss: 0.4735\n",
      "Epoch 036, Train Loss: 0.5167, Valid Loss: 0.4789\n",
      "Epoch 037, Train Loss: 0.5131, Valid Loss: 0.4757\n",
      "Epoch 038, Train Loss: 0.5146, Valid Loss: 0.4741\n",
      "Epoch 039, Train Loss: 0.5148, Valid Loss: 0.4758\n",
      "Epoch 040, Train Loss: 0.5145, Valid Loss: 0.4762\n",
      "Epoch 041, Train Loss: 0.5142, Valid Loss: 0.4784\n",
      "Epoch 042, Train Loss: 0.5136, Valid Loss: 0.4723\n",
      "Epoch 043, Train Loss: 0.5123, Valid Loss: 0.4736\n",
      "Epoch 044, Train Loss: 0.5140, Valid Loss: 0.4723\n",
      "Epoch 045, Train Loss: 0.5120, Valid Loss: 0.4725\n",
      "Epoch 046, Train Loss: 0.5120, Valid Loss: 0.4707\n",
      "Epoch 047, Train Loss: 0.5110, Valid Loss: 0.4736\n",
      "Epoch 048, Train Loss: 0.5104, Valid Loss: 0.4734\n",
      "Epoch 049, Train Loss: 0.5109, Valid Loss: 0.4746\n",
      "Epoch 050, Train Loss: 0.5096, Valid Loss: 0.4712\n",
      "Epoch 051, Train Loss: 0.5109, Valid Loss: 0.4726\n",
      "Epoch 052, Train Loss: 0.5097, Valid Loss: 0.4725\n",
      "Epoch 053, Train Loss: 0.5104, Valid Loss: 0.4724\n",
      "Epoch 054, Train Loss: 0.5095, Valid Loss: 0.4695\n",
      "Epoch 055, Train Loss: 0.5108, Valid Loss: 0.4715\n",
      "Epoch 056, Train Loss: 0.5093, Valid Loss: 0.4725\n",
      "Epoch 057, Train Loss: 0.5083, Valid Loss: 0.4695\n",
      "Epoch 058, Train Loss: 0.5081, Valid Loss: 0.4725\n",
      "Epoch 059, Train Loss: 0.5089, Valid Loss: 0.4696\n",
      "Epoch 060, Train Loss: 0.5084, Valid Loss: 0.4721\n",
      "Epoch 061, Train Loss: 0.5079, Valid Loss: 0.4719\n",
      "Epoch 062, Train Loss: 0.5084, Valid Loss: 0.4714\n",
      "Epoch 063, Train Loss: 0.5073, Valid Loss: 0.4717\n",
      "Epoch 064, Train Loss: 0.5058, Valid Loss: 0.4713\n",
      "Epoch 065, Train Loss: 0.5082, Valid Loss: 0.4694\n",
      "Epoch 066, Train Loss: 0.5063, Valid Loss: 0.4694\n",
      "Epoch 067, Train Loss: 0.5058, Valid Loss: 0.4716\n",
      "Epoch 068, Train Loss: 0.5050, Valid Loss: 0.4763\n",
      "Epoch 069, Train Loss: 0.5059, Valid Loss: 0.4704\n",
      "Epoch 070, Train Loss: 0.5043, Valid Loss: 0.4712\n",
      "Epoch 071, Train Loss: 0.5042, Valid Loss: 0.4753\n",
      "Epoch 072, Train Loss: 0.5070, Valid Loss: 0.4702\n",
      "Epoch 073, Train Loss: 0.5070, Valid Loss: 0.4759\n",
      "Epoch 074, Train Loss: 0.5064, Valid Loss: 0.4715\n",
      "Epoch 075, Train Loss: 0.5037, Valid Loss: 0.4725\n",
      "Epoch 076, Train Loss: 0.5032, Valid Loss: 0.4704\n",
      "Epoch 077, Train Loss: 0.4997, Valid Loss: 0.4710\n",
      "Epoch 078, Train Loss: 0.5038, Valid Loss: 0.4697\n",
      "Epoch 079, Train Loss: 0.5027, Valid Loss: 0.4707\n",
      "Epoch 080, Train Loss: 0.5028, Valid Loss: 0.4701\n",
      "Epoch 081, Train Loss: 0.5040, Valid Loss: 0.4691\n",
      "Epoch 082, Train Loss: 0.5024, Valid Loss: 0.4715\n",
      "Epoch 083, Train Loss: 0.5015, Valid Loss: 0.4706\n",
      "Epoch 084, Train Loss: 0.5033, Valid Loss: 0.4713\n",
      "Epoch 085, Train Loss: 0.5017, Valid Loss: 0.4697\n",
      "Epoch 086, Train Loss: 0.5037, Valid Loss: 0.4718\n",
      "Epoch 087, Train Loss: 0.5027, Valid Loss: 0.4694\n",
      "Epoch 088, Train Loss: 0.5014, Valid Loss: 0.4716\n",
      "Epoch 089, Train Loss: 0.5007, Valid Loss: 0.4702\n",
      "Epoch 090, Train Loss: 0.5007, Valid Loss: 0.4740\n",
      "Epoch 091, Train Loss: 0.5021, Valid Loss: 0.4700\n",
      "Epoch 092, Train Loss: 0.5007, Valid Loss: 0.4723\n",
      "Epoch 093, Train Loss: 0.5022, Valid Loss: 0.4700\n",
      "Epoch 094, Train Loss: 0.5007, Valid Loss: 0.4709\n",
      "Epoch 095, Train Loss: 0.5006, Valid Loss: 0.4699\n",
      "Epoch 096, Train Loss: 0.5012, Valid Loss: 0.4726\n",
      "Epoch 097, Train Loss: 0.5017, Valid Loss: 0.4686\n",
      "Epoch 098, Train Loss: 0.4994, Valid Loss: 0.4686\n",
      "Epoch 099, Train Loss: 0.4989, Valid Loss: 0.4705\n",
      "Epoch 100, Train Loss: 0.5004, Valid Loss: 0.4707\n",
      "‚úÖ GAT Training complete.\n",
      "‚öôÔ∏è Generating final GAT predictions...\n",
      "\n",
      "=== PyTorch Geometric GAT Evaluation ===\n",
      "\n",
      "üìä Multilabel Evaluation Metrics:\n",
      "Nom :               : Train (GNN GAT)\n",
      "Subset accuracy     : 0.0070\n",
      "Hamming loss        : 0.2742\n",
      "Micro F1            : 0.7737\n",
      "Macro F1            : 0.5771\n",
      "Weighted F1         : 0.7438\n",
      "Micro ROC-AUC       : 0.7817\n",
      "Macro ROC-AUC       : 0.5109\n",
      "\n",
      "üìä Multilabel Evaluation Metrics:\n",
      "Nom :               : Validation (GNN GAT)\n",
      "Subset accuracy     : 0.0350\n",
      "Hamming loss        : 0.2113\n",
      "Micro F1            : 0.8406\n",
      "Macro F1            : 0.6381\n",
      "Weighted F1         : 0.8131\n",
      "Micro ROC-AUC       : 0.8385\n",
      "Macro ROC-AUC       : 0.5604\n",
      "\n",
      "üìä Multilabel Evaluation Metrics:\n",
      "Nom :               : Test (GNN GAT)\n",
      "Subset accuracy     : 0.0210\n",
      "Hamming loss        : 0.2328\n",
      "Micro F1            : 0.8153\n",
      "Macro F1            : 0.6083\n",
      "Weighted F1         : 0.7938\n",
      "Micro ROC-AUC       : 0.8386\n",
      "Macro ROC-AUC       : 0.6142\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop (Reuse train_epoch, eval_model functions) ---\n",
    "n_epochs_gat = 100 # Start with 100, monitor validation loss\n",
    "\n",
    "print(f\"üöÄ Training PyTorch Geometric GAT for {n_epochs_gat} epochs...\")\n",
    "# Ensure train_epoch and eval_model functions are defined from previous cells\n",
    "\n",
    "best_valid_loss_gat = float('inf')\n",
    "patience_counter_gat = 0\n",
    "patience_gat = 20 # Early stopping patience\n",
    "\n",
    "for epoch in range(1, n_epochs_gat + 1):\n",
    "    train_loss = train_epoch(model_gat, train_loader_edge, criterion, optimizer_gat, device)\n",
    "    valid_loss, _, _ = eval_model(model_gat, valid_loader_edge, criterion, device)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    # Simple Early Stopping\n",
    "    if valid_loss < best_valid_loss_gat:\n",
    "        best_valid_loss_gat = valid_loss\n",
    "        patience_counter_gat = 0\n",
    "        # Optional: Save the best model state\n",
    "        # torch.save(model_gat.state_dict(), 'best_gat_model.pth')\n",
    "    else:\n",
    "        patience_counter_gat += 1\n",
    "\n",
    "    if patience_counter_gat >= patience_gat:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "        break\n",
    "\n",
    "print(\"‚úÖ GAT Training complete.\")\n",
    "# Optional: Load best model if saved\n",
    "# model_gat.load_state_dict(torch.load('best_gat_model.pth'))\n",
    "\n",
    "# --- Final Evaluation for GAT ---\n",
    "print(\"‚öôÔ∏è Generating final GAT predictions...\")\n",
    "_, train_logits_final_gat, _ = eval_model(model_gat, train_loader_edge, criterion, device)\n",
    "_, valid_logits_final_gat, _ = eval_model(model_gat, valid_loader_edge, criterion, device)\n",
    "_, test_logits_final_gat, _ = eval_model(model_gat, test_loader_edge, criterion, device)\n",
    "\n",
    "# Convert final logits to probabilities and predictions\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "y_train_prob_gat = sigmoid(train_logits_final_gat)\n",
    "y_valid_prob_gat = sigmoid(valid_logits_final_gat)\n",
    "y_test_prob_gat = sigmoid(test_logits_final_gat)\n",
    "\n",
    "y_train_pred_gat = (y_train_prob_gat >= 0.5).astype(int)\n",
    "y_valid_pred_gat = (y_valid_prob_gat >= 0.5).astype(int)\n",
    "y_test_pred_gat = (y_test_prob_gat >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== PyTorch Geometric GAT Evaluation ===\")\n",
    "# Ensure evaluate_multilabel_model function is defined\n",
    "metrics_train_gat = evaluate_multilabel_model(y_train_true, y_train_pred_gat, y_train_prob_gat, \"Train (GNN GAT)\")\n",
    "metrics_valid_gat = evaluate_multilabel_model(y_valid_true, y_valid_pred_gat, y_valid_prob_gat, \"Validation (GNN GAT)\")\n",
    "metrics_test_gat = evaluate_multilabel_model(y_test_true, y_test_pred_gat, y_test_prob_gat, \"Test (GNN GAT)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7abd47-7720-48b8-bb10-b47c180592e8",
   "metadata": {},
   "source": [
    "Let's make bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44601f8d-f1fb-49c7-bf8b-17c7aaa8a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 17:48:59,942] Using an existing study with name 'sider-gnn-gat-pyg-optimization' instead of creating a new one.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Optuna Bayesian Optimization for GAT (50 trials)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-23 17:49:52,995] Trial 1 finished with value: 0.6328113409179352 and parameters: {'lr': 0.000644397751444598, 'hidden_channels': 64, 'num_heads': 8, 'dense_dim': 256, 'dropout': 0.10972932408803958}. Best is trial 1 with value: 0.6328113409179352.\n"
     ]
    }
   ],
   "source": [
    "# --- Objective Function ---\n",
    "def objective_gat(trial):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    n_node_features = train_pyg_dataset_edge[0].num_node_features\n",
    "    n_edge_features = train_pyg_dataset_edge[0].num_edge_features\n",
    "    n_tasks = len(sider_tasks)\n",
    "\n",
    "    # Suggest Hyperparameters\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    hidden_channels = trial.suggest_categorical('hidden_channels', [64, 128, 256])\n",
    "    num_heads = trial.suggest_categorical('num_heads', [2, 4, 8, 16])\n",
    "    dense_dim = trial.suggest_categorical('dense_dim', [128, 256, 512])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.3)\n",
    "    n_epochs_objective = 100 # Fixed duration for trial\n",
    "\n",
    "    # Instantiate Model, Loss, Optimizer\n",
    "    model = SiderGAT_PyG( # Assumes SiderGAT_PyG class is defined\n",
    "        n_tasks=n_tasks, n_node_features=n_node_features, n_edge_features=n_edge_features,\n",
    "        hidden_channels=hidden_channels, num_heads=num_heads, dense_dim=dense_dim, dropout=dropout\n",
    "    ).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training Loop with simple early stopping\n",
    "    best_valid_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    patience = 15 # Stop after 15 epochs without validation loss improvement\n",
    "\n",
    "    for epoch in range(1, n_epochs_objective + 1):\n",
    "        train_loss = train_epoch(model, train_loader_edge, criterion, optimizer, device) # Assumes train_epoch defined\n",
    "        valid_loss, valid_logits, valid_labels = eval_model(model, valid_loader_edge, criterion, device) # Assumes eval_model defined\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        trial.report(valid_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            del model, optimizer, criterion; gc.collect(); torch.cuda.empty_cache()\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        if patience_counter >= patience:\n",
    "            break # Early stopping\n",
    "\n",
    "    # Evaluate on Validation Set (using last epoch's logits/labels from eval_model)\n",
    "    sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "    valid_probs = sigmoid(valid_logits)\n",
    "    valid_preds = (valid_probs >= 0.5).astype(int)\n",
    "    if valid_labels.ndim == 3 and valid_labels.shape[1] == 1: valid_labels = valid_labels.squeeze(1)\n",
    "    macro_f1_valid = f1_score(valid_labels, valid_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    # Clean up\n",
    "    del model, optimizer, criterion, valid_logits, valid_probs, valid_preds, valid_labels\n",
    "    gc.collect()\n",
    "    if device == 'cuda': torch.cuda.empty_cache()\n",
    "\n",
    "    return macro_f1_valid\n",
    "\n",
    "# --- Create and Run Study ---\n",
    "study_name_gat = \"sider-gnn-gat-pyg-optimization\"\n",
    "storage_name_gat = f\"sqlite:///{study_name_gat}.db\"\n",
    "study_gat = optuna.create_study(study_name=study_name_gat, storage=storage_name_gat, load_if_exists=True, direction=\"maximize\")\n",
    "n_trials_gat = 50 # Adjust as needed\n",
    "\n",
    "print(f\"üöÄ Starting Optuna Bayesian Optimization for GAT ({n_trials_gat} trials)...\")\n",
    "study_gat.optimize(objective_gat, n_trials=n_trials_gat)\n",
    "\n",
    "# --- Output Best Results ---\n",
    "print(\"\\n‚úÖ GAT Optimization Finished!\")\n",
    "print(\"Best GAT trial:\")\n",
    "best_trial_gat = study_gat.best_trial\n",
    "print(f\"  Value (Max Macro F1 on Validation): {best_trial_gat.value:.4f}\")\n",
    "print(\"  Params: \", best_trial_gat.params)\n",
    "\n",
    "# --- Remember to retrain the best model separately and evaluate on the test set ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cheminformatics_project)",
   "language": "python",
   "name": "cheminformatics_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

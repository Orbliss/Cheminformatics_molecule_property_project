{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679bdc4f",
   "metadata": {},
   "source": [
    "# Using transformers to predict binding affinity\n",
    "\n",
    "So far, we've tried methods using XGBoost on hand-engineering featurizations. We've then shown how a transformer can be trained on masked SMILES strings to (hopefully) build meaningful representations of them. Now, we want to combine these rich molecule embeddings with embeddings for the protein pockets, for which we will be using Meta's ESM2, which is proven to contain structural information despite being trained on sequences. The principles behind ESM2's training are very similar to how CuteSmileyBERT was trained, only done at a far larger scale.\n",
    "\n",
    "Our goal here will be to produce a neural network that takes in the embeddings of both the ligand and the pocket, and output a prediction for pKd. Our baseline will consist of concatenating both input vectors, then using a simple Multi-Layer Perceptron (MLP) architecture. This will be extremely helpful in telling us whether our embedding models encoded any relevant information, and if deep learning is a suitable solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f283ec15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcos/miniconda3/envs/cheminformatics_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import esm\n",
    "from rdkit import Chem\n",
    "from biopandas.pdb import PandasPdb\n",
    "from transformers import PretrainedConfig, PreTrainedModel, AutoTokenizer\n",
    "\n",
    "from src.transformer_classes import CuteSmileyBERT, CuteSmileyBERTConfig, SMILESTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0681299f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model checkpoint from Hugging Face Hub\n",
    "REPO = \"marcosbolanos/cutesmileybert-4.8m\" \n",
    "\n",
    "# We're defining the tokenizer locally for now\n",
    "# Hugging Face needed standardized definitions, no time to implement\n",
    "VOCAB_PATH = \"../data/vocab.json\"\n",
    "with open(VOCAB_PATH, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "inv_vocab = {v : k for k, v in vocab.items()}\n",
    "tokenizer = SMILESTokenizer(vocab, inv_vocab)\n",
    "\n",
    "# This is the model config, loaded from the Hugging Face Repo\n",
    "config = CuteSmileyBERTConfig.from_pretrained(REPO)\n",
    "# And this loads the model's weights\n",
    "ligand_embedder = CuteSmileyBERT.from_pretrained(REPO, config=config)\n",
    "\n",
    "# This is going to be our function to embed a single smiles string\n",
    "def get_smiles_pooled_embeddings(smiles: str): \n",
    "    # Tokenize the smiles string\n",
    "    encoded = tokenizer(smiles, return_tensors=\"pt\")\n",
    "    input_ids = encoded[\"input_ids\"]\n",
    "    # Feed the tokens into the embedder, recover the embeddings\n",
    "    with torch.no_grad():\n",
    "        emb = ligand_embedder(input_ids, return_embeddings=True)\n",
    "    \n",
    "    # Here, the embeddings are tensors containing 256-long column vectors for each token\n",
    "    # In practice, we'll mean pool the embeddings to get a gloabl representation as a single column\n",
    "    pooled_emb = emb.mean(dim=1)\n",
    "    return pooled_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "404cf8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load the test/train datasets, just like last time\n",
    "DATA_DIR = \"../data\"\n",
    "PDBBIND_DIR = Path(DATA_DIR, \"v2015\")\n",
    "INTERIM_DIR = Path(DATA_DIR, \"interim\")\n",
    "DATASET_PATH = Path(INTERIM_DIR, \"reg_preprocessed_1.npz\")\n",
    "\n",
    "data = np.load(DATASET_PATH)\n",
    "# We're going to do operations based on our list of train and test IDs\n",
    "# Each individual file will be loaded and embedded\n",
    "train_ids = data[\"train_ids\"]\n",
    "test_ids = data[\"test_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a256302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we're creating the loop to embed all of our train and test IDs\n",
    "def get_smiles_embeddings_list(pdb_ids: list[str]): \n",
    "    embeddings = []\n",
    "    for pdb_id in pdb_ids:\n",
    "        ligand_mol2_path = Path(PDBBIND_DIR, pdb_id, pdb_id + \"_ligand.mol2\")\n",
    "        # Make sure the file actually exists, otherwise skip\n",
    "        if not os.path.exists(ligand_mol2_path):\n",
    "            print(f'molecule {pdb_id} file not found')\n",
    "            continue\n",
    "        mol = Chem.MolFromMol2File(ligand_mol2_path, sanitize=False, removeHs=False)\n",
    "        # Again, we skip it if the molecule didn't load\n",
    "        if mol is None:\n",
    "            print(f'molecule {pdb_id} didnt load successfully')\n",
    "            continue\n",
    "        # Removing explicit hydrogens to match the format our model was trained on\n",
    "        mol = Chem.RemoveHs(mol, updateExplicitCount=True)\n",
    "        smiles = Chem.MolToSmiles(mol,\n",
    "                                  canonical=True,\n",
    "                                  isomericSmiles=False,\n",
    "                                  kekuleSmiles=False,\n",
    "                                  allHsExplicit=False)\n",
    "        smiles_emb = get_smiles_pooled_embeddings(smiles)\n",
    "        embeddings.append(smiles_emb)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956005ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYIELKLAGRWPVKVFIHNHKRYSAGERIVDIIATD\n"
     ]
    }
   ],
   "source": [
    "# Now we need embeddings for the protein pockets \n",
    "# For this, we're going to iterate through our dataset and get the sequences\n",
    "# We have three-letter AA names, so we'll map them to single letter\n",
    "aa_map = {\n",
    "    'ALA':'A', 'ARG':'R', 'ASN':'N', 'ASP':'D',\n",
    "    'CYS':'C', 'GLU':'E', 'GLN':'Q', 'GLY':'G',\n",
    "    'HIS':'H', 'ILE':'I', 'LEU':'L', 'LYS':'K',\n",
    "    'MET':'M', 'PHE':'F', 'PRO':'P', 'SER':'S',\n",
    "    'THR':'T', 'TRP':'W', 'TYR':'Y', 'VAL':'V',\n",
    "    'SEC':'U', 'PYL':'O'\n",
    "}\n",
    "\n",
    "# This function gives us the sequence string for a given PDB ID\n",
    "def get_pocket_sequence(pdb_id: str) -> str: \n",
    "    pocket_pdb_path = Path(PDBBIND_DIR, pdb_id, pdb_id + \"_pocket.pdb\")\n",
    "    if not Path.exists(pocket_pdb_path):\n",
    "        print(f'Warning: couldnt find pocket for complex {pdb_id}')\n",
    "        return None\n",
    "    ppdb = PandasPdb().read_pdb(pocket_pdb_path)\n",
    "    df = ppdb.df['ATOM']\n",
    "    df = df.drop_duplicates(subset='residue_number', keep='first')\n",
    "    df = df.sort_values(by=['chain_id', 'residue_number'])\n",
    "    seq = ''.join(aa_map.get(res, 'X') for res in df['residue_name'])\n",
    "    return seq\n",
    "\n",
    "sequence = get_pocket_sequence(test_ids[0])\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5396c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 640, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-29): 30 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (v_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (q_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (out_proj): Linear(in_features=640, out_features=640, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=640, out_features=2560, bias=True)\n",
       "      (fc2): Linear(in_features=2560, out_features=640, bias=True)\n",
       "      (final_layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=600, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=640, out_features=640, bias=True)\n",
       "    (layer_norm): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Now we load ESM2 thanks to the python package\n",
    "model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper will get pocket sequences and put them in the required format\n",
    "# ESM2 takes inputs as a list of ('name', SEQUENCE) tuples\n",
    "def get_esm_inputs_for_pdb_ids(pdb_ids:list[str]) -> tuple[list[tuple[str, str]], list[str]]: \n",
    "    missing_ids = []\n",
    "    esm_inputs = []\n",
    "    for pdb_id in pdb_ids:\n",
    "        seq = get_pocket_sequence(pdb_id)\n",
    "        if seq == None:\n",
    "            missing_ids.append(pdb_id)\n",
    "            continue\n",
    "        esm_inputs.append((str(pdb_id), seq))\n",
    "    return esm_inputs, missing_ids\n",
    "\n",
    "# Generate embeddings for a given list of PDB IDs\n",
    "def get_esm_embedding_list(pdb_ids:list[str]):\n",
    "    esm_inputs, missing_ids = get_esm_inputs_for_pdb_ids(pdb_ids)\n",
    "    # This is the tokenizer used by ESM2\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(esm_inputs)\n",
    "\n",
    "    # This gives us the last-layer token embeddings of each of our sequences\n",
    "    # return_contacts = False means we won't get the attention patterns\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[30], return_contacts=False)\n",
    "    token_embeddings = results[\"representations\"][30]\n",
    "\n",
    "    # We then mean pool the token embeddings for every sequence\n",
    "    # The result is a list of per-sequence embeddings, which we will fetch\n",
    "    sequence_embeddings = []\n",
    "    for i, (_, seq) in enumerate(esm_inputs):\n",
    "        sequence_embeddings.append(token_embeddings[i, 1 : len(seq) + 1].mean(0))\n",
    "    return sequence_embeddings, missing_ids\n",
    "\n",
    "train_esm_embeddings, missing_train_ids = get_esm_embedding_list(train_ids)\n",
    "test_esm_embeddings, missing_test_ids = get_esm_embedding_list(test_ids)\n",
    "\n",
    "print(len(train_esm_embeddings))\n",
    "print(len(test_esm_embeddings))\n",
    "print(train_esm_embeddings[0].shape)\n",
    "print(f'Missing IDs: train: {len(missing_train_ids)}, test: {len(missing_test_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9b519e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we know which IDs actually have protein pocket data\n",
    "# We can will remove those from our dataset and get SMILES embeddings for the remaining ones\n",
    "# None will be missing after this step\n",
    "retained_train_ids = train_ids - missing_train_ids\n",
    "retained_test_ids = test_ids - missing_test_ids\n",
    "\n",
    "train_smiles_embeddings = get_smiles_embeddings_list(retained_train_ids)\n",
    "test_smiles_embeddings = get_smiles_embeddings_list(retained_test_ids)\n",
    "\n",
    "print(len(train_smiles_embeddings))\n",
    "print(len(test_smiles_embeddings))\n",
    "print(train_smiles_embeddings[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e36a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concatenated_embeddings(smiles_embeddings, esm_embeddings):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheminformatics_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

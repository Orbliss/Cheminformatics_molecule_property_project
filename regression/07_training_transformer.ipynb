{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8682e29e",
   "metadata": {},
   "source": [
    "# CuteSmileyBert : a toy transformer for SMILES strings\n",
    "\n",
    "Chemical data has such underlying complexity that it is quite complicated to find representations of molecules that machine learning models can work with. Indeed, we could try different types of human-engineered featurizations, especially for the protein structures. However, they will all inevitably lose some information, or become noisy. For this reason, I believe that the best encoding for this specific task will be a BERT-like transformer of protein structures. ESM2 demonstrated an understanding of protein structures despite only being trained on sequences. We can definitely expect similar results on SMILES strings.\n",
    "\n",
    "CuteSmileyBert will have only 1 million parameters, which is absolutely **tiny**. I am not expecting it to work very well, but I am curious if it will work at all, and if we can demonstrate some form of scaling laws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78873962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0efd1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c215840d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m download_datasets, extract_files\n\u001b[32m     13\u001b[39m ROOT_DIR = Path(\u001b[33m\"\u001b[39m\u001b[33m../\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m DATA_DIR = Path(ROOT_DIR, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.download_dataset import download_datasets, extract_files\n",
    "\n",
    "ROOT_DIR = Path(\"../\")\n",
    "DATA_DIR = Path(ROOT_DIR, \"data\")\n",
    "\n",
    "# We start by downloading the dataset and opening it with Pandas\n",
    "smiles_datasets = {\n",
    "    'SMILES_Big_Dataset.csv.zip': 'https://www.kaggle.com/api/v1/datasets/download/yanmaksi/big-molecules-smiles-dataset'\n",
    "}\n",
    "\n",
    "download_datasets(smiles_datasets)\n",
    "extract_files(smiles_datasets)\n",
    "\n",
    "filename = list(smiles_datasets.keys())[0]\n",
    "file_path = Path(DATA_DIR, filename)\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07cd2a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1\n",
       "1    O=c1cc(-c2nc(-c3ccc(-c4cn(CCP(=O)(O)O)nn4)cc3)...\n",
       "2               NC(=O)c1ccc2c(c1)nc(C1CCC(O)CC1)n2CCCO\n",
       "3                  NCCCn1c(C2CCNCC2)nc2cc(C(N)=O)ccc21\n",
       "4                    CNC(=S)Nc1cccc(-c2cnc3ccccc3n2)c1\n",
       "Name: SMILES, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the column that we are interested in\n",
    "smiles_list = df[\"SMILES\"].to_list()\n",
    "\n",
    "df[\"SMILES\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc37f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of SMILES tokens that we're going to separate into lists\n",
    "SMILES_REGEX = re.compile(\n",
    "    r\"(\\%\\d\\d|Br|Cl|Si|Na|Ca|Li|@@?|=|#|\\(|\\)|\\.|\\[|\\]|\\/|\\\\|:|~|\\+|\\-|\\d|[A-Za-z])\"\n",
    ")\n",
    "\n",
    "# Special tokens to encode different types of empty or masked spaces\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "BOS_TOKEN = \"<BOS>\"\n",
    "EOS_TOKEN = \"<EOS>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN, MASK_TOKEN]\n",
    "\n",
    "# This is the max padded length of a \"sentence\"\n",
    "MAX_LEN = 150\n",
    "\n",
    "# Returns a list containing each individual token\n",
    "def tokenize_smiles(smiles: str) -> list[str]:\n",
    "    return SMILES_REGEX.findall(smiles)\n",
    "\n",
    "# Returns two dictionaries mapping unique tokens to integer indexes\n",
    "def build_vocab(smiles_list: list[str]) -> tuple[dict[str, int], dict[int, str]]:\n",
    "    # Create a set of tokens (sets don't contain repeated values)\n",
    "    tokens = set(SPECIAL_TOKENS)\n",
    "    for s in smiles_list:\n",
    "        tokens.update(tokenize_smiles(s))\n",
    "    # Sort all tokens, make a list, and map each token to its index\n",
    "    vocab = {tok: i for i, tok in enumerate(sorted(tokens))}\n",
    "    # Reverse dictionary that maps indexes to tokens\n",
    "    inv_vocab = {i: tok for tok, i in vocab.items()}\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "# Turn smiles string into fixed-length list of token IDs, with special tokens\n",
    "def encode_smiles(\n",
    "    smiles: str, \n",
    "    vocab: dict[str, int], \n",
    "    max_len: int=MAX_LEN\n",
    ") -> list[int]:\n",
    "    # Adds start and end tokens to the SMILES list\n",
    "    tokens = [BOS_TOKEN] + tokenize_smiles(smiles) + [EOS_TOKEN]\n",
    "    # Creates a list with the integer IDs of each token\n",
    "    token_ids = [vocab.get(t, vocab[UNK_TOKEN]) for t in tokens]\n",
    "    # Create a vector of length max_len and fill it with pad tokens until the end\n",
    "    token_ids = token_ids[:max_len] + [vocab[PAD_TOKEN]] * (max_len - len(token_ids))\n",
    "    return token_ids\n",
    "\n",
    "# Get a list of token IDs as input, replace some by <MASK>\n",
    "def mask_tokens(\n",
    "    input_ids: list[int], \n",
    "    vocab: dict[str, int], \n",
    "    mask_prob: float=0.15\n",
    ") -> tuple[list[int], list[int]]:\n",
    "    # Two copies of ID vector so we can mask one and predict the other\n",
    "    input_ids = input_ids.clone()\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    # Get the IDs of mask and pad tokens, plus context size\n",
    "    mask_token_id = vocab[MASK_TOKEN]\n",
    "    pad_token_id = vocab[PAD_TOKEN]\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Do not mask padding tokens\n",
    "    maskable = input_ids != pad_token_id\n",
    "    masked_indices = torch.bernoulli(torch.full(input_ids.shape, mask_prob)).bool() & maskable\n",
    "    labels[~masked_indices] = -100\n",
    "\n",
    "    # Replace 80% of tokens with <MASK>\n",
    "    replace_mask = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n",
    "    input_ids[replace_mask] = mask_token_id\n",
    "\n",
    "    # Replace 10% with random token\n",
    "    random_mask = torch.bernoulli(torch.full(input_ids.shape, 0.1)).bool() & masked_indices & ~replace_mask\n",
    "    random_tokens = torch.randint(vocab_size, input_ids.shape, dtype=torch.long)\n",
    "    input_ids[random_mask] = random_tokens[random_mask]\n",
    "\n",
    "    # 10% keep unchanged (no-op for the rest)\n",
    "    return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "830b02ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class with input-target pairs for masked vs original strings\n",
    "class SMILESMaskedDataset(torch.utils.data.Dataset):\n",
    "    # Creates class attribute \"data\", a list of every encoded SMILES string\n",
    "    def __init__(\n",
    "        self, \n",
    "        smiles_list: list[str], \n",
    "        vocab: dict[str, int], \n",
    "        max_len: int=150\n",
    "    ) -> None:\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.data = [encode_smiles(s, vocab, max_len) for s in smiles_list]\n",
    "\n",
    "    # Length method, required by pytorch\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    # Returns a pair of masked and unmasked encodings in Tensor form\n",
    "    def __getitem__(self, idx:int) -> tuple[list[int], list[int]]:\n",
    "        input_ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        masked_input, labels = mask_tokens(input_ids, self.vocab)\n",
    "        return masked_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Transformer class\n",
    "class CuteSmileyBERT(nn.Module):\n",
    "    def __init__(self, \n",
    "        vocab_size: int, \n",
    "        d_model: int=256, \n",
    "        nhead: int=8, \n",
    "        num_layers: int=6, \n",
    "        dim_feedforward: int=1024, \n",
    "        dropout: float=0.1, \n",
    "        max_len: int=150\n",
    "    ) -> None:\n",
    "        # Intialize the nn.Module parent class, needed for proper inheritance\n",
    "        super().__init__()\n",
    "        # Set the input max_len to a class attribute\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # This defines a method for embeddings, which will be our input representation\n",
    "        # It effectively creates a weight matrix of shape [vocab_size, d_model]\n",
    "        # The weights are initialized at random and will be learnt over time\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        # We will also be using positional embeddings, which will be added to the tokens'\n",
    "        # They will be learnt based solely on integer positions, with no information on the tokens\n",
    "        self.pos_embed = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # Defines a single transfomer layer, with attention and feedforward/MLP layers\n",
    "        # Each later contains nhead attention heads, and a ff with dimensions dim_feedforward\n",
    "        # Dropout prevents overfitting by randomly setting 10% of attention coefficients to 0\n",
    "        # Batch-first means that the input shape is given as [batch, seq_len, d_model]\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "\n",
    "        # This attribute defines the model's main structure, a sequence of num_layers encoder layers\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # This is the final layer, a simple linear MLP which maps the embeddings to token logits\n",
    "        # The input dimension is d_model, and the output is vocab_size (1-hot encoded vector)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    # This is a single training cycle. \n",
    "    def forward(self, input_ids:list[int]):\n",
    "        # Get batch size and sequence length, used for positional encodings\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds maximum {self.max_len}\")\n",
    "        \n",
    "        # Creates a 1D tensor which simply contains the int positions of each token\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        # Our positions tensor is embedded, without any information on the actual tokens\n",
    "        pos_embeddings = self.pos_embed(positions)\n",
    "        # The positional embeddings are then summed to the token embeddings\n",
    "        x = self.embed(input_ids) + pos_embeddings\n",
    "\n",
    "        # We pass the embeddings through the encoder block\n",
    "        x = self.encoder(x)\n",
    "        # The linear layer outputs logits\n",
    "        logits = self.lm_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60966512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we're going to wrap our model inside a Hugging Face Transformers class\n",
    "# This gives us a standard definition that can be easily pushed and pulled\n",
    "\n",
    "from transformers import PretrainedConfig, PreTrainedModel\n",
    "\n",
    "class CSBConfig(PretrainedConfig):\n",
    "    model_type = \"CuteSmileyBERT\"\n",
    "    def __init__(self, vocab_size=100, d_model=256, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.1, max_len=150, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.num_layers = num_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.dropout = dropout\n",
    "        self.max_len = max_len\n",
    "\n",
    "class CuteSmileyBERT_HF(PreTrainedModel):\n",
    "    config_class = CuteSmileyBERTConfig\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = CuteSmileyBERT(\n",
    "            vocab_size=config.vocab_size,\n",
    "            d_model=config.d_model,\n",
    "            nhead=config.nhead,\n",
    "            num_layers=config.num_layers,\n",
    "            dim_feedforward=config.dim_feedforward,\n",
    "            dropout=config.dropout,\n",
    "            max_len=config.max_len\n",
    "        )\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        return self.model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec9c7b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "No CUDA device detected.\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device:\", torch.cuda.current_device())\n",
    "    print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"No CUDA device detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "vocab, inv_vocab = build_vocab(smiles_list)\n",
    "\n",
    "dataset = SMILESMaskedDataset(smiles_list, vocab, max_len=150)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "config = CuteSmileyBERTConfig(vocab_size=len(vocab))\n",
    "hf_model = CuteSmileyBERT_HF(config)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for masked_input, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = hf_model(masked_input)\n",
    "        loss = criterion(logits.view(-1, len(vocab)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbb977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save in Hugging Face format\n",
    "hf_model.save_pretrained(\"./cutesmileybert\")\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "hf_model.push_to_hub(\"marcosbolanos/cutesmileybert\")\n",
    "config.push_to_hub(\"marcosbolanos/cutesmileybert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237778eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheminformatics_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

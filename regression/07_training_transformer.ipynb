{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8682e29e",
   "metadata": {},
   "source": [
    "# CuteSmileyBert : a toy transformer for SMILES strings\n",
    "\n",
    "Chemical data has such underlying complexity that it is quite complicated to find representations of molecules that machine learning models can work with. Indeed, we could try different types of human-engineered featurizations, especially for the protein structures. However, they will all inevitably lose some information, or become noisy. For this reason, I believe that the best encoding for this specific task will be a BERT-like transformer of protein structures. ESM2 demonstrated an understanding of protein structures despite only being trained on sequences. We can definitely expect similar results on SMILES strings.\n",
    "\n",
    "CuteSmileyBert will have only 1 million parameters, which is absolutely **tiny**. I am not expecting it to work very well, but I am curious if it will work at all, and if we can demonstrate some form of scaling laws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78873962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c215840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES_Big_Dataset.csv.zip est déjà présent.\n",
      "Extraction de SMILES_Big_Dataset.csv.zip...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SMILES_Big_Dataset.csv.zip: 100%|██████████| 1/1 [00:00<00:00, 38.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMILES_Big_Dataset.csv.zip extrait dans /var/home/marcos/Code/Cheminformatics_molecule_property_project/data/SMILES_Big_Dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from src.download_dataset import download_datasets, extract_files\n",
    "\n",
    "ROOT_DIR = Path(\"../\")\n",
    "DATA_DIR = Path(ROOT_DIR, \"data\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the SMILES dataset to download\n",
    "smiles_datasets = {\n",
    "    'SMILES_Big_Dataset.csv.zip': 'https://www.kaggle.com/api/v1/datasets/download/yanmaksi/big-molecules-smiles-dataset'\n",
    "}\n",
    "\n",
    "# Download the dataset\n",
    "download_datasets(smiles_datasets)\n",
    "extract_files(smiles_datasets)\n",
    "\n",
    "filename = list(smiles_datasets.keys())[0]\n",
    "file_path = Path(DATA_DIR, filename)\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "07cd2a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           O=S(=O)(Nc1cccc(-c2cnc3ccccc3n2)c1)c1cccs1\n",
       "1    O=c1cc(-c2nc(-c3ccc(-c4cn(CCP(=O)(O)O)nn4)cc3)...\n",
       "2               NC(=O)c1ccc2c(c1)nc(C1CCC(O)CC1)n2CCCO\n",
       "3                  NCCCn1c(C2CCNCC2)nc2cc(C(N)=O)ccc21\n",
       "4                    CNC(=S)Nc1cccc(-c2cnc3ccccc3n2)c1\n",
       "Name: SMILES, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the column that we are interested in\n",
    "smiles_list = df[\"SMILES\"].to_list()\n",
    "\n",
    "df[\"SMILES\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc37f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_smiles(smiles: str):\n",
    "    return SMILES_REGEX.findall(smiles)\n",
    "\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "SPECIAL_TOKENS = [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN, UNK_TOKEN, MASK_TOKEN]\n",
    "\n",
    "def build_vocab(smiles_list):\n",
    "    tokens = set(SPECIAL_TOKENS)\n",
    "    for s in smiles_list:\n",
    "        tokens.update(tokenize_smiles(s))\n",
    "    vocab = {tok: i for i, tok in enumerate(sorted(tokens))}\n",
    "    inv_vocab = {i: tok for tok, i in vocab.items()}\n",
    "    return vocab, inv_vocab\n",
    "\n",
    "MAX_LEN = 150\n",
    "\n",
    "def encode_smiles(smiles, vocab, max_len=MAX_LEN):\n",
    "    tokens = [BOS_TOKEN] + tokenize_smiles(smiles) + [EOS_TOKEN]\n",
    "    token_ids = [vocab.get(t, vocab[UNK_TOKEN]) for t in tokens]\n",
    "    token_ids = token_ids[:max_len] + [vocab[PAD_TOKEN]] * (max_len - len(token_ids))\n",
    "    return token_ids\n",
    "\n",
    "def mask_tokens(input_ids, vocab, mask_prob=0.15):\n",
    "    input_ids = input_ids.clone()\n",
    "    labels = input_ids.clone()\n",
    "\n",
    "    mask_token_id = vocab[MASK_TOKEN]\n",
    "    pad_token_id = vocab[PAD_TOKEN]\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Do not mask padding tokens\n",
    "    maskable = input_ids != pad_token_id\n",
    "    masked_indices = torch.bernoulli(torch.full(input_ids.shape, mask_prob)).bool() & maskable\n",
    "    labels[~masked_indices] = -100  # ignore loss on unmasked tokens\n",
    "\n",
    "    # 80% replace with <MASK>\n",
    "    replace_mask = torch.bernoulli(torch.full(input_ids.shape, 0.8)).bool() & masked_indices\n",
    "    input_ids[replace_mask] = mask_token_id\n",
    "\n",
    "    # 10% replace with random token\n",
    "    random_mask = torch.bernoulli(torch.full(input_ids.shape, 0.1)).bool() & masked_indices & ~replace_mask\n",
    "    random_tokens = torch.randint(vocab_size, input_ids.shape, dtype=torch.long)\n",
    "    input_ids[random_mask] = random_tokens[random_mask]\n",
    "\n",
    "    # 10% keep unchanged (no-op for the rest)\n",
    "    return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830b02ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 149]) torch.Size([4, 149])\n"
     ]
    }
   ],
   "source": [
    "class SMILESMaskedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, smiles_list, vocab, max_len=150):\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.data = [encode_smiles(s, vocab, max_len) for s in smiles_list]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data[idx], dtype=torch.long)\n",
    "        masked_input, labels = mask_tokens(input_ids, self.vocab)\n",
    "        return masked_input, labels\n",
    "    \n",
    "vocab, inv_vocab = build_vocab(smiles_list)\n",
    "\n",
    "dataset = SMILESDataset(smiles_list, vocab, max_len=150)\n",
    "loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "for src, tgt in loader:\n",
    "    print(src.shape, tgt.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SMILESBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=6, dim_feedforward=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        x = self.embed(input_ids)\n",
    "        x = self.encoder(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, inv_vocab = build_vocab(smiles_list)\n",
    "\n",
    "dataset = SMILESMaskedDataset(smiles_list, vocab, max_len=150)\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = SMILESBERT(len(vocab))\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for masked_input, labels in loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(masked_input)\n",
    "        loss = criterion(logits.view(-1, len(vocab)), labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}: loss = {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheminformatics_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

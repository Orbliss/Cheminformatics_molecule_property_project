{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679bdc4f",
   "metadata": {},
   "source": [
    "# Using transformers to predict binding affinity\n",
    "\n",
    "So far, we've tried methods using XGBoost on hand-engineering featurizations. We've then shown how a transformer can be trained on masked SMILES strings to (hopefully) build meaningful representations of them. Now, we want to combine these rich molecule embeddings with embeddings for the protein pockets, for which we will be using Meta's ESM3. The principles behind ESM3's training are very similar to how CuteSmileyBERT was trained, only done at a far larger scale.\n",
    "\n",
    "Our goal here will be to produce a neural network that takes in the embeddings of both the ligand and the pocket, and output a prediction for pKd. Our baseline will consist of concatenating both input vectors, then using a simple Multi-Layer Perceptron (MLP) architecture. This will be extremely helpful in telling us whether our embedding models encoded any relevant information, and if deep learning is a suitable solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f283ec15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcos/miniconda3/envs/cheminformatics_project/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from transformers import PretrainedConfig, PreTrainedModel, AutoTokenizer\n",
    "\n",
    "from src.transformer_classes import CuteSmileyBERT, CuteSmileyBERTConfig, SMILESTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0681299f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 256])\n"
     ]
    }
   ],
   "source": [
    "# Loading the model checkpoint from Hugging Face Hub\n",
    "REPO = \"marcosbolanos/cutesmileybert-4.8m\" \n",
    "\n",
    "# We're defining the tokenizer locally for now\n",
    "# Hugging Face needed standardized definitions, no time to implement\n",
    "VOCAB_PATH = \"../data/vocab.json\"\n",
    "with open(VOCAB_PATH, \"r\") as f:\n",
    "    vocab = json.load(f)\n",
    "inv_vocab = {v : k for k, v in vocab.items()}\n",
    "tokenizer = SMILESTokenizer(vocab, inv_vocab)\n",
    "\n",
    "# This is the model config, loaded from the Hugging Face Repo\n",
    "config = CuteSmileyBERTConfig.from_pretrained(REPO)\n",
    "# And this loads the model's weights\n",
    "model = CuteSmileyBERT.from_pretrained(REPO, config=config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval()\n",
    "\n",
    "encoded = tokenizer(\"CCCCCC\", return_tensors=\"pt\") if callable(tokenizer) else {\"input_ids\": torch.tensor([tokenizer.encode(\"CCO\")])}\n",
    "input_ids = encoded[\"input_ids\"].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    emb = model(input_ids, return_embeddings=True)  # use your return_embeddings flag\n",
    "print(emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956005ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cheminformatics_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
